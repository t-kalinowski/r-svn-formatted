\input texinfo
@c %**start of header
@setfilename R-FAQ.info
@settitle R FAQ
@setchapternewpage on
@set FAQ_YEAR 2005
@set FAQ_DATE @value{FAQ_YEAR}-06-21
@set REL_YEAR 2005
@set REL_MAJOR 2
@set REL_MINOR 1
@set REL_PATCHLEVEL 1
@set REL_VERSION @value{REL_MAJOR}.@value{REL_MINOR}.@value{REL_PATCHLEVEL}
@set FAQ_VERSION @value{REL_MAJOR}.@value{REL_MINOR}.@value{FAQ_DATE}

@c <COMMENT>
@c ISBNs for R 1.x.
@c @set FAQ_ISBN 3-900051-01-1
@c @set R_ISBN 3-900051-00-3
@c </COMMENT>
@set FAQ_ISBN 3-900051-08-9
@set R_ISBN 3-900051-07-0
@documentlanguage en
@documentencoding ISO-8859-1
@c %**end of header

@dircategory Programming
@direntry
* R FAQ: (R-FAQ).               The R statistical system FAQ.
@end direntry

@finalout

@macro SPLUS{}
@sc{S-Plus}
@end macro

@macro CRAN{}
@acronym{CRAN}
@end macro

@macro HTML{}
@acronym{HTML}
@end macro

@macro FORTRAN{}
FORTRAN
@end macro

@macro XML{}
@acronym{XML}
@end macro

@macro XSL{}
@acronym{XSL}
@end macro

@titlepage
@title R @acronym{FAQ}
@subtitle Frequently Asked Questions on R
@subtitle Version @value{FAQ_VERSION}
@subtitle ISBN @value{FAQ_ISBN}
@author Kurt Hornik
@end titlepage

@ifinfo
@c We do not really see this in info, but in plain text output.
R FAQ                            @*
Frequently Asked Questions on R  @*
Version @value{FAQ_VERSION}      @*
ISBN @value{FAQ_ISBN}            @*
Kurt Hornik                      @*

@sp 2
@end ifinfo

@ifnothtml
@contents
@end ifnothtml

@ifnottex
@node Top, Introduction, (dir), (dir)
@top R FAQ
@ifhtml
@html
<h2>Frequently Asked Questions on R</h2>
<h2>Version @value{FAQ_VERSION}</h2>
<h2>ISBN @value{FAQ_ISBN}</h2>
<address>Kurt Hornik</address>
<p><p><hr><p>
@end html
@end ifhtml
@end ifnottex

@menu
* Introduction::                
* R Basics::                    
* R and S::                     
* R Web Interfaces::            
* R Add-On Packages::           
* R and Emacs::                 
* R Miscellanea::               
* R Programming::               
* R Bugs::                      
* Acknowledgments::             
@end menu

@node Introduction, R Basics, Top, Top
@chapter Introduction

This document contains answers to some of the most frequently asked
questions about R.

@menu
* Legalese::                    
* Obtaining this document::     
* Citing this document::        
* Notation::                    
* Feedback::                    
@end menu

@node Legalese, Obtaining this document, Introduction, Introduction
@section Legalese

This document is copyright @copyright{} 1998--@value{FAQ_YEAR} by Kurt
Hornik.

This document is free software; you can redistribute it and/or modify it
under the terms of the @acronym{GNU} General Public License as published
by the Free Software Foundation; either version 2, or (at your option)
any later version.

This document is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
@acronym{GNU} General Public License for more details.

A copy of the @acronym{GNU} General Public License is available via WWW
at

@display
@url{http://www.gnu.org/copyleft/gpl.html}.
@end display

@noindent
You can also obtain it by writing to the Free Software Foundation, Inc.,
59 Temple Place --- Suite 330, Boston, MA 02111-1307, USA.

@node Obtaining this document, Citing this document, Legalese, Introduction
@section Obtaining this document

The latest version of this document is always available from

@display
@url{http://CRAN.R-project.org/doc/FAQ/}
@end display

From there, you can obtain versions converted to
@url{http://CRAN.R-project.org/doc/FAQ/R-FAQ.txt,, plain
@acronym{ASCII} text},
@url{http://CRAN.R-project.org/doc/FAQ/R-FAQ.dvi.gz,, DVI},
@url{http://CRAN.R-project.org/doc/FAQ/R-FAQ.info.gz,, @acronym{GNU}
info}, @url{http://CRAN.R-project.org/doc/FAQ/R-FAQ.html,, @HTML{}},
@url{http://CRAN.R-project.org/doc/FAQ/R-FAQ.pdf,, PDF},
@url{http://CRAN.R-project.org/doc/FAQ/R-FAQ.ps.gz,, PostScript} as
well as the @url{http://CRAN.R-project.org/doc/FAQ/R-FAQ.texi,,
Texinfo source} used for creating all these formats using the
@url{http://texinfo.org/, @acronym{GNU} Texinfo system}.

You can also obtain the R @acronym{FAQ} from the @file{doc/FAQ}
subdirectory of a @CRAN{} site (@pxref{What is CRAN?}).

@node Citing this document, Notation, Obtaining this document, Introduction
@section Citing this document

In publications, please refer to this @acronym{FAQ} as Hornik
(@value{FAQ_YEAR}), ``The R @acronym{FAQ}'', and give the above,
@emph{official} @acronym{URL} and the ISBN @value{FAQ_ISBN}.

@node Notation, Feedback, Citing this document, Introduction
@section Notation

Everything should be pretty standard.  @samp{R>} is used for the R
prompt, and a @samp{$} for the shell prompt (where applicable).

@node Feedback,  , Notation, Introduction
@section Feedback

Feedback via email to @email{Kurt.Hornik@@R-project.org} is of course
most welcome.

In particular, note that I do not have access to Windows or Macintosh
systems.  Features specific to the Windows and Mac OS X ports of R are
described in the
@url{http://cran.r-project.org/bin/windows/base/rw-FAQ.html, ``R for
Windows @acronym{FAQ}''} and the
@url{http://cran.R-project.org/bin/macosx/RMacOSX-FAQ.html, ``R for Mac
OS X @acronym{FAQ}}.  If you have information on Macintosh or Windows
systems that you think should be added to this document, please let me
know.

@c <FIXME>
@c Should we maybe have direct links inside the R tree to the various
@c rw-FAQ versions?
@c </FIXME>

@node R Basics, R and S, Introduction, Top
@chapter R Basics

@menu
* What is R?::                  
* What machines does R run on?::  
* What is the current version of R?::  
* How can R be obtained?::      
* How can R be installed?::     
* Are there Unix binaries for R?::  
* What documentation exists for R?::  
* Citing R::                    
* What mailing lists exist for R?::  
* What is CRAN?::               
* Can I use R for commercial purposes?::  
* Why is R named R?::           
* What is the R Foundation?::   
@end menu

@node What is R?, What machines does R run on?, R Basics, R Basics
@section What is R?

R is a system for statistical computation and graphics.  It consists of
a language plus a run-time environment with graphics, a debugger, access
to certain system functions, and the ability to run programs stored in
script files.

The design of R has been heavily influenced by two existing languages:
Becker, Chambers & Wilks' S (@pxref{What is S?}) and Sussman's
@url{http://www.cs.indiana.edu/scheme-repository/home.html, Scheme}.
Whereas the resulting language is very similar in appearance to S, the
underlying implementation and semantics are derived from Scheme.
@xref{What are the differences between R and S?}, for further details.

The core of R is an interpreted computer language which allows branching
and looping as well as modular programming using functions.  Most of the
user-visible functions in R are written in R.  It is possible for the
user to interface to procedures written in the C, C++, or FORTRAN
languages for efficiency.  The R distribution contains functionality for
a large number of statistical procedures.  Among these are: linear and
generalized linear models, nonlinear regression models, time series
analysis, classical parametric and nonparametric tests, clustering and
smoothing.  There is also a large set of functions which provide a
flexible graphical environment for creating various kinds of data
presentations.  Additional modules (``add-on packages'') are available
for a variety of specific purposes (@pxref{R Add-On Packages}).

R was initially written by @email{Ross.Ihaka@@R-project.org, Ross Ihaka}
and @email{Robert.Gentleman@@R-project.org, Robert Gentleman} at the
Department of Statistics of the University of Auckland in Auckland, New
Zealand.  In addition, a large group of individuals has contributed to R
by sending code and bug reports.

Since mid-1997 there has been a core group (the ``R Core Team'') who can
modify the R source code archive.  The group currently consists of Doug
Bates, John Chambers, Peter Dalgaard, Robert Gentleman, Kurt Hornik,
Stefano Iacus, Ross Ihaka, Friedrich Leisch, Thomas Lumley, Martin
Maechler, Duncan Murdoch, Paul Murrell, Martyn Plummer, Brian Ripley,
Duncan Temple Lang, Luke Tierney, and Simon Urbanek.

R has a home page at @url{http://www.R-project.org/}.  It is free
software distributed under a @acronym{GNU}-style copyleft, and an
official part of the @acronym{GNU} project (``@acronym{GNU} S'').

@node What machines does R run on?, What is the current version of R?, What is R?, R Basics
@section What machines does R run on?

R is being developed for the Unix, Windows and Mac families of operating
systems.  Support for Mac OS Classic ended with R 1.7.1.

The current version of R will configure and build under a number of
common Unix platforms including @var{cpu}-linux-gnu for the i386, alpha,
arm, hppa, ia64, m68k, mips/mipsel, powerpc, s390, sparc (e.g.,
@url{http://buildd.debian.org/build.php?&pkg=r-base}), and x86_64 CPUs,
powerpc-apple-darwin, mips-sgi-irix,
@c <FIXME>
@c Not sure anymore ...
@c alpha-dec-osf4,
@c i386-freebsd,
@c i386-sun-solaris,
@c hppa-hp-hpux, 
@c </FIXME>
rs6000-ibm-aix, and sparc-sun-solaris.

@c and according to @email{jlindsey@@luc.ac.be, Jim Lindsey} also on
@c Mac, Amiga and Atari under m68k-linux.

If you know about other platforms, please drop us a note.

@node What is the current version of R?, How can R be obtained?, What machines does R run on?, R Basics
@section What is the current version of R?
 
The current released version is @value{REL_VERSION}.  Based on this
`major.minor.patchlevel' numbering scheme, there are two development
versions of R, a patched version of the current release (`r-patched')
and one working towards the next minor or eventually major (`r-devel')
releases of R, respectively.  Version r-patched is for bug fixes mostly.
New features are typically introduced in r-devel.

@node How can R be obtained?, How can R be installed?, What is the current version of R?, R Basics
@section How can R be obtained?

Sources, binaries and documentation for R can be obtained via @CRAN{},
the ``Comprehensive R Archive Network'' (see @ref{What is CRAN?}).

Sources are also available via @url{https://svn.r-project.org/R/}, the
R Subversion repository, but currently not via anonymous rsync (nor
CVS).

Tarballs with daily snapshots of the r-devel and r-patched development
versions of R can be found at
@url{ftp://ftp.stat.math.ethz.ch/Software/R}.

@c Sources are also available via anonymous rsync.  Use

@c @example
@c rsync -rptC --delete rsync.R-project.org::@var{module} R
@c @end example

@c @noindent
@c to create a copy of the source tree specified by @var{module} in the
@c subdirectory @file{R} of the current directory, where @var{module}
@c specifies one of the three existing flavors of the R sources, and can be
@c one of @samp{r-release} (current released version), @samp{r-patched}
@c (patched released version), and @samp{r-devel} (development version).
@c The rsync trees are created directly from the master CVS archive and are
@c updated hourly.  The @option{-C} and in the @command{rsync} command
@c is to cause it to skip the CVS directories.  Further information on
@c @command{rsync} is available at @url{http://rsync.samba.org/rsync/}.

@c @c <NOTE>
@c @c Keep in sync with R-admin.
@c Note that the sources available via rsync do not include the recommended
@c packages, whereas these are included in the tarballs of released
@c versions.  To install the appropriate sources for the recommended
@c packages, run @command{./tools/rsync-recommended} from the top-level of
@c the R sources that you pulled by rsync.
@c @c </NOTE>

@c The sources of the development version are also available via anonymous
@c CVS.  See @url{http://anoncvs.R-project.org} for more information.

@node How can R be installed?, Are there Unix binaries for R?, How can R be obtained?, R Basics
@section How can R be installed?

@menu
* How can R be installed (Unix)::  
* How can R be installed (Windows)::  
* How can R be installed (Macintosh)::  
@end menu

@node How can R be installed (Unix), How can R be installed (Windows), How can R be installed?, How can R be installed?
@subsection How can R be installed (Unix)

If R is already installed, it can be started by typing @kbd{R} at the
shell prompt (of course, provided that the executable is in your path).

If binaries are available for your platform (see @ref{Are there Unix
binaries for R?}), you can use these, following the instructions that
come with them.

Otherwise, you can compile and install R yourself, which can be done
very easily under a number of common Unix platforms (see @ref{What
machines does R run on?}).  The file @file{INSTALL} that comes with the
R distribution contains a brief introduction, and the ``R Installation
and Administration'' guide (@pxref{What documentation exists for R?})
has full details.

Note that you need a @FORTRAN{} compiler or perhaps @command{f2c} in
addition to a C compiler to build R.  Also, you need Perl version 5 to
build the R object documentations.  (If this is not available on your
system, you can obtain a PDF version of the object reference manual via
@CRAN{}.)

In the simplest case, untar the R source code, change to the directory
thus created, and issue the following commands (at the shell prompt):

@example
$ ./configure
$ make
@end example

If these commands execute successfully, the R binary and a shell script
front-end called @file{R} are created and copied to the @file{bin}
directory.  You can copy the script to a place where users can invoke
it, for example to @file{/usr/local/bin}.  In addition, plain text help
pages as well as @HTML{} and @LaTeX{} versions of the documentation are
built.

Use @kbd{make dvi} to create DVI versions of the R manuals, such as
@file{refman.dvi} (an R object reference index) and @file{R-exts.dvi},
the ``R Extension Writers Guide'', in the @file{doc/manual}
subdirectory.  These files can be previewed and printed using standard
programs such as @command{xdvi} and @command{dvips}.  You can also use
@kbd{make pdf} to build PDF (Portable Document Format) version of the
manuals, and view these using e.g.@: Acrobat.  Manuals written in the
@acronym{GNU} Texinfo system can also be converted to info files
suitable for reading online with Emacs or stand-alone @acronym{GNU}
Info; use @kbd{make info} to create these versions (note that this
requires Makeinfo version 4.5).

Finally, use @kbd{make check} to find out whether your R system works
correctly.

You can also perform a ``system-wide'' installation using @kbd{make
install}.  By default, this will install to the following directories:

@table @file
@item $@{prefix@}/bin
the front-end shell script
@item $@{prefix@}/man/man1
the man page
@item $@{prefix@}/lib/R
all the rest (libraries, on-line help system, @dots{}).  This is the ``R
Home Directory'' (@env{R_HOME}) of the installed system.
@end table

@noindent
In the above, @code{prefix} is determined during configuration
(typically @file{/usr/local}) and can be set by running
@command{configure} with the option

@example
$ ./configure --prefix=/where/you/want/R/to/go
@end example

@noindent
(E.g., the R executable will then be installed into
@file{/where/you/want/R/to/go/bin}.)

To install DVI, info and PDF versions of the manuals, use @kbd{make
install-dvi}, @kbd{make install-info} and @kbd{make install-pdf},
respectively.

@node How can R be installed (Windows), How can R be installed (Macintosh), How can R be installed (Unix), How can R be installed?
@subsection How can R be installed (Windows)

The @file{bin/windows} directory of a @CRAN{} site contains binaries for
a base distribution and a large number of add-on packages from @CRAN{}
to run on Windows 95, 98, ME, NT4, 2000, and XP (at least) on Intel and
clones (but not on other platforms).  The Windows version of R was
created by Robert Gentleman and Guido Masarotto, and is now being
developed and maintained by @email{murdoch@@stats.uwo.ca, Duncan
Murdoch} and @email{Brian.Ripley@@R-project.org, Brian D. Ripley}.

@c Note that when uncompressing the zip files, the pkunzip program needs to
@c be invoked with the @samp{-D} flag to create subdirectories.  Also, be
@c aware that some decompression programs do not preserve long file names
@c properly.

For most installations the Windows installer program will be the easiest
tool to use.

See the @url{http://www.stats.ox.ac.uk/pub/R/rw-FAQ.html, ``R for
Windows @acronym{FAQ}''} for more details.

@node How can R be installed (Macintosh),  , How can R be installed (Windows), How can R be installed?
@subsection How can R be installed (Macintosh)

The @file{bin/macosx} directory of a @CRAN{} site contains a standard
Apple installer package named @file{RAqua.pkg.sit} compressed in Aladdin
Stuffit format.  Once downloaded, uncompressed and executed, the
installer will install the current non-developer release of R.  RAqua is
a native Mac OS X Darwin version of R with an Aqua GUI.  Inside
@file{bin/macosx/@var{x}.@var{y}} there are prebuilt binary packages to
be used with RAqua corresponding to the ``@var{x}.@var{y}'' release of
R. The installation of these packages is available through the
``Package'' menu of the RAqua GUI.  This port of R for Mac OS X is
maintained by @email{Stefano.Iacus@@R-project.org, Stefano Iacus}.  The
@url{http://cran.R-project.org/bin/macosx/RMacOSX-FAQ.html, ``R for
Mac OS X @acronym{FAQ}} has more details.

The @file{bin/macos} directory of a @CRAN{} site contains bin-hexed
(@file{hqx}) and stuffit (@file{sit}) archives for a base distribution
and a large number of add-on packages of R 1.7.1 to run under Mac OS 8.6
to Mac OS 9.2.2.  This port of R for Macintosh is no longer supported.

@node Are there Unix binaries for R?, What documentation exists for R?, How can R be installed?, R Basics
@section Are there Unix binaries for R?

@c Linux binaries as of 2005-04-21:
@c
@c debian
@c   stable  i386   2.1.0  Christian T. Steigies <cts@debian.org>
@c mandrake
@c   10.0    i386   2.0.0  Michele Alzetta <mikalzet@libero.it>
@c redhat
@c   rh8     i386   2.0.0  Martyn Plummer <plummer@iarc.fr>
@c   rh9     i386   2.0.0  Martyn Plummer <plummer@iarc.fr>
@c   fc1     i386   2.0.0  Martyn Plummer <plummer@iarc.fr>
@c           x86_64 1.8.1  James Henstridge <james@daa.com.au>
@c   fc2     i386   2.0.0  Martyn Plummer <plummer@iarc.fr>
@c   fc3     i386   2.0.1  Martyn Plummer <plummer@iarc.fr>
@c           x86_64 2.0.1  Brian Ripley
@c   el3     i386   2.0.1  Matthew P Cox
@c   el4     i386   2.0.1  Matthew P Cox
@c suse
@c   7.3     i386   1.8.0  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c   8.0     i386   1.8.1  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c   8.1     i386   1.9.1  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c   8.2     i386   2.0.0  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c   9.0     i586   2.0.0  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c   9.1     i586   2.0.1  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c   9.2     i586   2.1.0  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c           x86_64 2.1.0  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c   9.3     i586   2.1.0  Detlef Steuer <Detlef.Steuer@unibw-hamburg.de>
@c vinelinux
@c   3.0     i386   2.0.1  Susunu Tanimura <stanimura-ngs@umin.ac.jp>

The @file{bin/linux} directory of a @CRAN{} site contains the following
packages.

@quotation
@multitable {VineLinux} {x86_64} {8.x/9/Fedora1/Fedora2/Fedora3} {James Henstridge}
@headitem @tab CPU @tab Versions @tab Provider
@item Debian   @tab i386 @tab stable @tab Christian Steigies
@item Mandrake @tab i386 @tab 10.0 @tab Michele Alzetta
@item Red Hat  @tab i386 @tab 8/9/Fedora1/Fedora2/Fedora3 @tab Martyn
Plummer
@item          @tab x86_64 @tab Fedora1 @tab James Henstridge
@item          @tab x86_64 @tab Fedora3 @tab Brian Ripley
@item          @tab i386   @tab Enterprise Linux @tab Matthew P. Cox
@item SuSE     @tab i386   @tab 7.3/8.0/8.1/8.2 @tab Detlef Steuer
@item          @tab i586   @tab 9.0/9.1/9.2/9.3 @tab Detlef Steuer
@item          @tab x86_64 @tab 9.2 @tab Detlef Steuer
@item VineLinux @tab i386 @tab 3.0 @tab Susunu Tanimura
@end multitable
@end quotation

@c Mandrake 9.1/9.2/10.0 i386 packages by Michele Alzetta, Red Hat
@c 8.x/9/Fedora1/Fedora2/Fedora3 i386, by Martyn Plummer, Fedora1 x86_64,
@c and Fedora3 x86_64 packages by Martyn Plummer, James Henstridge, and
@c Brian Ripley respectively, SuSE 7.3/8.0/8.1/8.2 i386 and 9.0/9.1/9.2
@c i586 packages by Detlef Steuer, and VineLinux 2.6 i386 packages by
@c Susunu Tanimura.

Debian packages, maintained by Dirk Eddelbuettel and Doug Bates, have
long been part of the Debian distribution, and can be accessed through
APT, the Debian package maintenance tool.  Use e.g.@: @code{apt-get
install r-base r-recommended} to install the R environment and
recommended packages.  If you also want to build R packages from source,
also run @code{apt-get install r-base-dev} to obtain the additional
tools required for this.  So-called ``backports'' of the current R
packages for the @dfn{stable} distribution are provided by Christian
Steigies, and available from CRAN.  Simply add the line

@example
deb http://cran.R-project.org/bin/linux/debian stable
@end example

@noindent
(feel free to use a @CRAN{} mirror instead of the master) to the file
@file{/etc/apt/sources.list}, and install as usual.

@c Once you have added that line the programs @command{apt-get},
@c @command{apt-cache}, and @command{dselect} (using the apt access
@c method) will automatically detect and install updates of the R
@c packages.

@c The @file{bin/osf} directory of a @CRAN{} site contains RPMs
@c by Albrecht Gebhardt for alpha systems running Alpha Unix
@c (OSF/Tru64).

@c There are also `tar' distributions for NEXTSTEP on the i386 and m68k
@c platforms in @file{bin/nextstep/i386} and @file{bin/nextstep/m68k},
@c created by Stephen Shiboski <steve@biostat.ucsf.edu>.

No other binary distributions are currently publically available.

@node What documentation exists for R?, Citing R, Are there Unix binaries for R?, R Basics
@section What documentation exists for R?

Online documentation for most of the functions and variables in R
exists, and can be printed on-screen by typing @kbd{help(@var{name})}
(or @kbd{?@var{name}}) at the R prompt, where @var{name} is the name of
the topic help is sought for.  (In the case of unary and binary
operators and control-flow special forms, the name may need to be be
quoted.)

This documentation can also be made available as one reference manual
for on-line reading in @HTML{} and PDF formats, and as hardcopy via
@LaTeX{}, see @ref{How can R be installed?}.  An up-to-date @HTML{}
version is always available for web browsing at
@url{http://stat.ethz.ch/R-manual/}.

Printed copies of the R reference manual for some version(s) are
available from Network Theory Ltd, at
@url{http://www.network-theory.co.uk/R/base/}.  For each set of manuals
sold, the publisher donates USD 10 to the R Foundation (@pxref{What is
the R Foundation?}).

The R distribution also comes with the following manuals.

@itemize @bullet
@item ``An Introduction to R'' (@file{R-intro})
includes information on data types, programming elements, statistical
modeling and graphics.  This document is based on the ``Notes on
@SPLUS{}'' by Bill Venables and David Smith.
@item ``Writing R Extensions'' (@file{R-exts})
currently describes the process of creating R add-on packages, writing R
documentation, R's system and foreign language interfaces, and the R
@acronym{API}.
@item ``R Data Import/Export'' (@file{R-data})
is a guide to importing and exporting data to and from R.
@item ``The R Language Definition'' (@file{R-lang}),
a first version of the ``Kernighan & Ritchie of R'', explains
evaluation, parsing, object oriented programming, computing on the
language, and so forth.
@item ``R Installation and Administration'' (@file{R-admin}).
@end itemize

Books on R include

@quotation
P. Dalgaard (2002), ``Introductory Statistics with R'', Springer: New
York, ISBN 0-387-95475-9.@*
@url{http://www.biostat.ku.dk/~pd/ISwR.html}.

J. Fox (2002), ``An R and @SPLUS{} Companion to Applied Regression'',
Sage Publications, ISBN 0-761-92280-6 (softcover) or 0-761-92279-2
(hardcover),@*
@url{http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/}.

J. Maindonald and J. Braun (2003), ``Data Analysis and Graphics Using R:
An Example-Based Approach'', Cambridge University Press, ISBN
0-521-81336-0,@*
@url{http://wwwmaths.anu.edu.au/~johnm/}.

S. M. Iacus and G. Masarotto (2002), ``Laboratorio di statistica con
R'', McGraw-Hill, ISBN 88-386-6084-0 (in Italian),@*
@url{http://www.ateneonline.it/LibroAteneo.asp?item_id=1436}.

P. Murrell (2005), ``R Graphics'', Chapman & Hall/CRC, ISBN:
1-584-88486-X,@*
@url{http://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html}.

@end quotation

@noindent
The book

@quotation
W. N. Venables and B. D. Ripley (2002), ``Modern Applied Statistics with
S.  Fourth Edition''.  Springer, ISBN 0-387-95457-0
@end quotation

@noindent
has a home page at @url{http://www.stats.ox.ac.uk/pub/MASS4/} providing
additional material.  Its companion is

@quotation
W. N. Venables and B. D. Ripley (2000), ``S Programming''.  Springer,
ISBN 0-387-98966-8
@end quotation

@noindent
and provides an in-depth guide to writing software in the S language
which forms the basis of both the commercial @SPLUS{} and the Open
Source R data analysis software systems.  See
@url{http://www.stats.ox.ac.uk/pub/MASS3/Sprog/} for more information.

In addition to material written specifically or explicitly for R,
documentation for S/@SPLUS{} (see @ref{R and S}) can be used in
combination with this @acronym{FAQ} (@pxref{What are the differences
between R and S?}).  Introductory books include

@quotation
P. Spector (1994), ``An introduction to S and @SPLUS{}'', Duxbury Press.

A. Krause and M. Olsen (2002), ``The Basics of @SPLUS{}'' (Third
Edition).  Springer, ISBN 0-387-95456-2
@end quotation

The book

@quotation
J. C. Pinheiro and D. M. Bates (2000), ``Mixed-Effects Models in S and
@SPLUS{}'', Springer, ISBN 0-387-98957-0
@end quotation

@noindent
provides a comprehensive guide to the use of the @strong{nlme} package
for linear and nonlinear mixed-effects models.
@c This has a home page at @url{http://nlme.stat.wisc.edu/MEMSS/}.

As an example of how R can be used in teaching an advanced introductory
statistics course, see

@quotation
D. Nolan and T. Speed (2000), ``Stat Labs: Mathematical Statistics
Through Applications'', Springer Texts in Statistics, ISBN
0-387-98974-9
@end quotation

@noindent
This integrates theory of statistics with the practice of statistics
through a collection of case studies (``labs''), and uses R to analyze
the data.  More information can be found at
@url{http://www.stat.Berkeley.EDU/users/statlabs/}.

Last, but not least, Ross' and Robert's experience in designing and
implementing R is described in Ihaka & Gentleman (1996), ``R: A Language
for Data Analysis and Graphics'',
@url{http://www.amstat.org/publications/jcgs/, , @emph{Journal of
Computational and Graphical Statistics}}, @strong{5}, 299--314.

An annotated bibliography (Bib@TeX{} format) of R-related publications
which includes most of the above references can be found at

@display
@url{http://www.R-project.org/doc/bib/R.bib}
@end display

@node Citing R, What mailing lists exist for R?, What documentation exists for R?, R Basics
@section Citing R

To cite R in publications, use

@example
@group
@@Manual@{,
  title        = @{R: A Language and Environment for Statistical
                  Computing@},
  author       = @{@{R Development Core Team@}@},
  organization = @{R Foundation for Statistical Computing@},
  address      = @{Vienna, Austria@},
  year         = @value{REL_YEAR},
  note         = @{@{ISBN@} @value{R_ISBN}@},
  url          = @{http://www.R-project.org@}
@}
@end group
@end example

Citation strings (or Bib@TeX{} entries) for R and R packages can also be
obtained by @code{citation()}.

@node What mailing lists exist for R?, What is CRAN?, Citing R, R Basics
@section What mailing lists exist for R?

Thanks to @email{Martin.Maechler@@R-project.org, Martin Maechler}, there
are four mailing lists devoted to R.

@table @code
@item R-announce
A moderated list for major announcements about the development of R and
the availability of new code.
@item R-packages
A moderated list for announcements on the availability of new or
enhanced contributed packages.
@item R-help
The `main' R mailing list, for discussion about problems and solutions
using R, announcements (not covered by `R-announce' and `R-packages')
about the development of R and the availability of new code.
@c enhancements and patches to the source code and documentation of R,
@c comparison and compatibility with S and @SPLUS{}, and for the posting of
@c nice examples and benchmarks.
@item R-devel
This list is for questions and discussion about code development in R.
@c discussions about the future of R, proposals of new functionality, and
@c pre-testing of new versions.  It is meant for those who maintain an
@c active position in the development of R.
@end table

@noindent
Please read the @url{http://www.R-project.org/posting-guide.html,
posting guide} @emph{before} sending anything to any mailing list.

Note in particular that R-help is intended to be comprehensible to
people who want to use R to solve problems but who are not necessarily
interested in or knowledgeable about programming.  Questions likely to
prompt discussion unintelligible to non-programmers (e.g., questions
involving C or C++) should go to R-devel.

Convenient access to information on these lists, subscription, and
archives is provided by the web interface at
@url{http://stat.ethz.ch/mailman/listinfo/}.  One can also subscribe
(or unsubscribe) via email, e.g.@: to R-help by sending @samp{subscribe}
(or @samp{unsubscribe}) in the @emph{body} of the message (not in the
subject!) to @email{R-help-request@@lists.R-project.org}.

Send email to @email{R-help@@lists.R-project.org} to send a message to
everyone on the R-help mailing list.  Subscription and posting to the
other lists is done analogously, with @samp{R-help} replaced by
@samp{R-announce}, @samp{R-packages}, and @samp{R-devel}, respectively.
Note that the R-announce and R-packages lists are gatewayed into R-help.
Hence, you should subscribe to either of them only in case you are not
subscribed to R-help.

It is recommended that you send mail to R-help rather than only to the R
Core developers (who are also subscribed to the list, of course).  This
may save them precious time they can use for constantly improving R, and
will typically also result in much quicker feedback for yourself.

Of course, in the case of bug reports it would be very helpful to have
code which reliably reproduces the problem.  Also, make sure that you
include information on the system and version of R being used.  See
@ref{R Bugs} for more details.

See @url{http://www.R-project.org/mail.html} for more information on
the R mailing lists.

The R Core Team can be reached at @email{R-core@@lists.R-project.org}
for comments and reports.

@node What is CRAN?, Can I use R for commercial purposes?, What mailing lists exist for R?, R Basics
@section What is @acronym{CRAN}?

The ``Comprehensive R Archive Network'' (@CRAN{}) is a collection of
sites which carry identical material, consisting of the R
distribution(s), the contributed extensions, documentation for R, and
binaries.

The @CRAN{} master site at TU Wien, Austria, can be found at the
@acronym{URL}

@quotation
@c @multitable @columnfractions .45 .30
@c @item
@url{http://cran.R-project.org/}
@c @tab (Austria)
@c @end multitable
@end quotation

@noindent
@c (which is the same as @url{http://cran.at.R-project.org/})
Daily mirrors are available at @acronym{URL}s including

@quotation
@multitable @columnfractions .45 .40
@item @url{http://cran.at.R-project.org/}
@tab (TU Wien, Austria)
@item @url{http://cran.au.R-project.org/}
@tab (PlanetMirror, Australia)
@item @url{http://cran.br.R-project.org/}
@tab (Universidade Federal de Paran@'a, Brazil)
@item @url{http://cran.ch.R-project.org/}
@tab (ETH Z@"urich, Switzerland)
@item @url{http://cran.dk.R-project.org/}
@tab (SunSITE, Denmark)
@item @url{http://cran.es.R-project.org/}
@tab (Spanish National Research Network, Madrid, Spain)
@item @url{http://cran.fr.R-project.org/}
@tab (INRA, Toulouse, France)
@item @url{http://cran.hu.R-project.org/}
@tab (Semmelweis U, Hungary)
@item @url{http://cran.pt.R-project.org/}
@tab (Universidade do Porto, Portugal)
@item @url{http://cran.uk.R-project.org/}
@tab (U of Bristol, United Kingdom)
@item @url{http://cran.us.R-project.org/}
@tab (pair Networks, USA)
@item @url{http://cran.za.R-project.org/}
@tab (Rhodes U, South Africa)
@end multitable
@end quotation

@noindent
See @url{http://cran.R-project.org/mirrors.html} for a complete list of
mirrors.  Please use the @CRAN{} site closest to you to reduce network
load.

From @CRAN{}, you can obtain the latest official release of R, daily
snapshots of R (copies of the current source trees), as gzipped and
bzipped tar files, a wealth of additional contributed code, as well as
prebuilt binaries for various operating systems (Linux, Mac OS Classic,
Mac OS X, and MS Windows).  @CRAN{} also provides access to
documentation on R, existing mailing lists and the R Bug Tracking
system.

To ``submit'' to @CRAN{}, simply upload to
@url{ftp://cran.R-project.org/incoming/} and send an email to
@email{cran@@R-project.org}.  Note that @CRAN{} generally does not
accept submissions of precompiled binaries due to security reasons.  In
particular, binary packages for Windows and Mac OS X are provided by the
respective binary package maintainers.

@quotation Note
It is very important that you indicate the copyright (license)
information (@acronym{GPL}, @acronym{BSD}, Artistic, @dots{}) in your
submission.
@end quotation

Please always use the @acronym{URL} of the master site when referring to
@CRAN{}.

@node Can I use R for commercial purposes?, Why is R named R?, What is CRAN?, R Basics
@section Can I use R for commercial purposes?

R is released under the @url{http://www.gnu.org/copyleft/gpl.html,,
@acronym{GNU} General Public License (GPL)}.  If you have any questions
regarding the legality of using R in any particular situation you should
bring it up with your legal counsel.  We are in no position to offer
legal advice.

It is the opinion of the R Core Team that one can use R for commercial
purposes (e.g., in business or in consulting).  The GPL, like all Open
Source licenses, permits all and any use of the package.  It only
restricts distribution of R or of other programs containing code from R.
This is made clear in clause 6 (``No Discrimination Against Fields of
Endeavor'') of the @url{http://www.opensource.org/docs/definition.html,
Open Source Definition}:

@quotation
The license must not restrict anyone from making use of the program in a
specific field of endeavor.  For example, it may not restrict the
program from being used in a business, or from being used for genetic
research.
@end quotation

@noindent
It is also explicitly stated in clause 0 of the GPL, which says in part

@quotation
Activities other than copying, distribution and modification are not
covered by this License; they are outside its scope.  The act of running
the Program is not restricted, and the output from the Program is
covered only if its contents constitute a work based on the Program.
@end quotation

Most add-on packages, including all recommended ones, also explicitly
allow commercial use in this way.  A few packages are restricted to
``non-commercial use''; you should contact the author to clarify whether
these may be used or seek the advice of your legal counsel.

None of the discussion in this section constitutes legal advice.  The R
Core Team does not provide legal advice under any circumstances.

@node Why is R named R?, What is the R Foundation?, Can I use R for commercial purposes?, R Basics
@section Why is R named R?

The name is partly based on the (first) names of the first two R authors
(Robert Gentleman and Ross Ihaka), and partly a play on the name of the
Bell Labs language `S' (@pxref{What is S?}).

@c At the time the name was coined no one expected that the software would
@c get used outside of Auckland, so it seemed ok to make a joke of it.

@node What is the R Foundation?,  , Why is R named R?, R Basics
@section What is the R Foundation?

The R Foundation is a not for profit organization working in the public
interest.  It was founded by the members of the R Core Team in order to
provide support for the R project and other innovations in statistical
computing, provide a reference point for individuals, institutions or
commercial enterprises that want to support or interact with the R
development community, and to hold and administer the copyright of R
software and documentation.  See
@url{http://www.R-project.org/foundation/} for more information.

@node R and S, R Web Interfaces, R Basics, Top
@chapter R and S

@menu
* What is S?::                  
* What is S-PLUS?::             
* What are the differences between R and S?::  
* Is there anything R can do that S-PLUS cannot?::  
* What is R-plus?::             
@end menu

@node What is S?, What is S-PLUS?, R and S, R and S
@section What is S?

S is a very high level language and an environment for data analysis and
graphics.  In 1998, the Association for Computing Machinery
(@acronym{ACM}) presented its Software System Award to John M. Chambers,
the principal designer of S, for

@quotation
the S system, which has forever altered the way people analyze,
visualize, and manipulate data @dots{}

S is an elegant, widely accepted, and enduring software system, with
conceptual integrity, thanks to the insight, taste, and effort of John
Chambers.
@end quotation

The evolution of the S language is characterized by four books by John
Chambers and coauthors, which are also the primary references for S.

@itemize @bullet
@item
Richard A. Becker and John M. Chambers (1984), ``S.  An Interactive
Environment for Data Analysis and Graphics,'' Monterey: Wadsworth and
Brooks/Cole.

This is also referred to as the ``@emph{Brown Book}'', and of historical
interest only.

@item
Richard A. Becker, John M. Chambers and Allan R. Wilks (1988), ``The New
S Language,'' London: Chapman & Hall.

This book is often called the ``@emph{Blue Book}'', and introduced what
is now known as S version 2.

@item
John M. Chambers and Trevor J. Hastie (1992), ``Statistical Models in
S,''  London: Chapman & Hall.

This is also called the ``@emph{White Book}'', and introduced S version
3, which added structures to facilitate statistical modeling in S.

@item
John M. Chambers (1998), ``Programming with Data,'' New York: Springer,
ISBN 0-387-98503-4
(@url{http://cm.bell-labs.com/cm/ms/departments/sia/Sbook/}).

This ``@emph{Green Book}'' describes version 4 of S, a major revision of
S designed by John Chambers to improve its usefulness at every stage of
the programming process.
@end itemize

See @url{http://cm.bell-labs.com/cm/ms/departments/sia/S/history.html}
for further information on ``Stages in the Evolution of S''.

There is a huge amount of user-contributed code for S, available at the
@url{http://lib.stat.cmu.edu/S/, S Repository} at @acronym{CMU}.

@c The @url{http://lib.stat.cmu.edu/S/faq, ``Frequently Asked Questions
@c about S''} contains further information about S, but is not
@c up-to-date.

@node What is S-PLUS?, What are the differences between R and S?, What is S?, R and S
@section What is @sc{S-Plus}?

@SPLUS{} is a value-added version of S sold by Insightful Corporation.
Based on the S language, @SPLUS{} provides functionality in a wide
variety of areas, including robust regression, modern non-parametric
regression, time series, survival analysis, multivariate analysis,
classical statistical tests, quality control, and graphics drivers.
Add-on modules add additional capabilities.

See the @url{http://www.insightful.com/products/splus/, Insightful
@SPLUS{} page} for further information.

@node What are the differences between R and S?, Is there anything R can do that S-PLUS cannot?, What is S-PLUS?, R and S
@section What are the differences between R and S?

We can regard S as a language with three current implementations or
``engines'', the ``old S engine'' (S version 3; @SPLUS{} 3.x and 4.x),
the ``new S engine'' (S version 4; @SPLUS{} 5.x and above), and R.
Given this understanding, asking for ``the differences between R and S''
really amounts to asking for the specifics of the R implementation of
the S language, i.e., the difference between the R and S @emph{engines}.

For the remainder of this section, ``S'' refers to the S engines and not
the S language.

@menu
* Lexical scoping::             
* Models::                      
* Others::                      
@end menu

@node Lexical scoping, Models, What are the differences between R and S?, What are the differences between R and S?
@subsection Lexical scoping

Contrary to other implementations of the S language, R has adopted an
evaluation model in which nested function definitions are lexically
scoped.  This is analogous to the evalutation model in Scheme.

This difference becomes manifest when @emph{free} variables occur in a
function.  Free variables are those which are neither formal parameters
(occurring in the argument list of the function) nor local variables
(created by assigning to them in the body of the function).  In S, the
values of free variables are determined by a set of global variables
(similar to C, there is only local and global scope).  In R, they are
determined by the environment in which the function was created.

Consider the following function:

@example
@group
cube <- function(n) @{
  sq <- function() n * n
  n * sq()
@}
@end group
@end example

Under S, @code{sq()} does not ``know'' about the variable @code{n}
unless it is defined globally:

@example
@group
S> cube(2)
Error in sq():  Object "n" not found
Dumped
S> n <- 3
S> cube(2)
[1] 18
@end group
@end example

In R, the ``environment'' created when @code{cube()} was invoked is
also looked in:

@example
@group
R> cube(2)
[1] 8
@end group
@end example

@c The following more `realistic' example illustrating the differences in
@c scoping is due to @email{tlumley@@u.washington.edu, Thomas Lumley}.
@c The function

@c @example
@c jackknife.lm <- function(lmobj) @{
@c   n <- length(resid(lmobj))
@c   jval <- sapply(1:n, function(i) coef(update(lmobj, subset = -i)))
@c   (n - 1) * (n - 1) * var(jval) / n
@c @}
@c @end example

@c @noindent
@c does something useful in R, but does not work in S.  In order to make it
@c work in S you need to explicitly pass the linear model object into the
@c function nested in @code{apply()}.  If you don't and you are lucky you
@c will get @samp{Error: Object "lmobj" not found}.  If you are unlucky
@c enough to have a linear model called @code{lmobj} in your global
@c environment you will get the wrong answer with no warning.

@c The following version works in S.

@c @example
@c jackknife.S.lm <- function(lmobj) @{
@c   n <- length(resid(lmobj))
@c   jval <- sapply(1:n,
@c                  function(i, lmobj) coef(update(lmobj, subset = -i)), 
@c                  lmobj = lmobj)
@c   (n - 1) * (n - 1) * var(jval) / n
@c @}
@c @end example

@c (The S version was written independently by Thomas and at least three of
@c his fellow students over the past couple of years, causing literally
@c hours of confusion on each occasion.)

As a more ``interesting'' real-world problem, suppose you want to write
a function which returns the density function of the @math{r}-th order
statistic from a sample of size @math{n} from a (continuous)
distribution.  For simplicity, we shall use both the cdf and pdf of the
distribution as explicit arguments.  (Example compiled from various
postings by Luke Tierney.)

The @SPLUS{} documentation for @code{call()} basically suggests the
following:

@example
@group
dorder <- function(n, r, pfun, dfun) @{
  f <- function(x) NULL
  con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))
  PF <- call(substitute(pfun), as.name("x"))
  DF <- call(substitute(dfun), as.name("x"))
  f[[length(f)]] <-
    call("*", con,
         call("*", call("^", PF, r - 1),
              call("*", call("^", call("-", 1, PF), n - r),
                   DF)))
  f
@}
@end group
@end example

@noindent Rather tricky, isn't it?  The code uses the fact that in S,
functions are just lists of special mode with the function body as the
last argument, and hence does not work in R (one could make the idea
work, though).

A version which makes heavy use of @code{substitute()} and seems to work
under both S and R is

@example
@group
dorder <- function(n, r, pfun, dfun) @{
  con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))
  eval(substitute(function(x) K * PF(x)^a * (1 - PF(x))^b * DF(x),
                  list(PF = substitute(pfun), DF = substitute(dfun),
                       a = r - 1, b = n - r, K = con)))
@}
@end group
@end example

@noindent
(the @code{eval()} is not needed in S).

However, in R there is a much easier solution:

@example
@group
dorder <- function(n, r, pfun, dfun) @{
  con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))
  function(x) @{
    con * pfun(x)^(r - 1) * (1 - pfun(x))^(n - r) * dfun(x)
  @}
@}
@end group
@end example

@noindent
This seems to be the ``natural'' implementation, and it works because
the free variables in the returned function can be looked up in the
defining environment (this is lexical scope).

Note that what you really need is the function @emph{closure}, i.e., the
body along with all variable bindings needed for evaluating it.  Since
in the above version, the free variables in the value function are not
modified, you can actually use it in S as well if you abstract out the
closure operation into a function @code{MC()} (for ``make closure''):

@example
@group
dorder <- function(n, r, pfun, dfun) @{
  con <- round(exp(lgamma(n + 1) - lgamma(r) - lgamma(n - r + 1)))
  MC(function(x) @{
       con * pfun(x)^(r - 1) * (1 - pfun(x))^(n - r) * dfun(x)
     @},
     list(con = con, pfun = pfun, dfun = dfun, r = r, n = n))
@}
@end group
@end example

Given the appropriate definitions of the closure operator, this works in
both R and S, and is much ``cleaner'' than a substitute/eval solution
(or one which overrules the default scoping rules by using explicit
access to evaluation frames, as is of course possible in both R and S).

For R, @code{MC()} simply is

@example
MC <- function(f, env) f
@end example

@noindent (lexical scope!), a version for S is

@example
@group
MC <- function(f, env = NULL) @{
  env <- as.list(env)
  if (mode(f) != "function")
    stop(paste("not a function:", f))
  if (length(env) > 0 && any(names(env) == ""))
    stop(paste("not all arguments are named:", env))
  fargs <- if(length(f) > 1) f[1:(length(f) - 1)] else NULL
  fargs <- c(fargs, env)
  if (any(duplicated(names(fargs))))
    stop(paste("duplicated arguments:", paste(names(fargs)),
         collapse = ", "))
  fbody <- f[length(f)]
  cf <- c(fargs, fbody)
  mode(cf) <- "function"
  return(cf)
@}
@end group
@end example

Similarly, most optimization (or zero-finding) routines need some
arguments to be optimized over and have other parameters that depend on
the data but are fixed with respect to optimization.  With R scoping
rules, this is a trivial problem; simply make up the function with the
required definitions in the same environment and scoping takes care of
it.  With S, one solution is to add an extra parameter to the function
and to the optimizer to pass in these extras, which however can only
work if the optimizer supports this.

Nested lexically scoped functions allow using function closures and
maintaining local state.  A simple example (taken from Abelson and
Sussman) is obtained by typing @kbd{demo("scoping")} at the R prompt.
Further information is provided in the standard R reference ``R: A
Language for Data Analysis and Graphics'' (@pxref{What documentation
exists for R?}) and in Robert Gentleman and Ross Ihaka (2000), ``Lexical
Scope and Statistical Computing'',
@url{http://www.amstat.org/publications/jcgs/, , @emph{Journal of
Computational and Graphical Statistics}}, @strong{9}, 491--508.

Nested lexically scoped functions also imply a further major difference.
Whereas S stores all objects as separate files in a directory somewhere
(usually @file{.Data} under the current directory), R does not.  All
objects in R are stored internally.  When R is started up it grabs a
piece of memory and uses it to store the objects.  R performs its own
memory management of this piece of memory, growing and shrinking its
size as needed.  Having everything in memory is necessary because it is
not really possible to externally maintain all relevant ``environments''
of symbol/value pairs.  This difference also seems to make R
@emph{faster} than S.

The down side is that if R crashes you will lose all the work for the
current session.  Saving and restoring the memory ``images'' (the
functions and data stored in R's internal memory at any time) can be a
bit slow, especially if they are big.  In S this does not happen,
because everything is saved in disk files and if you crash nothing is
likely to happen to them.  (In fact, one might conjecture that the S
developers felt that the price of changing their approach to persistent
storage just to accommodate lexical scope was far too expensive.)
Hence, when doing important work, you might consider saving often (see
@ref{How can I save my workspace?}) to safeguard against possible
crashes.  Other possibilities are logging your sessions, or have your R
commands stored in text files which can be read in using
@code{source()}.

@quotation Note
If you run R from within Emacs (see @ref{R and Emacs}), you can save the
contents of the interaction buffer to a file and conveniently manipulate
it using @code{ess-transcript-mode}, as well as save source copies of
all functions and data used.
@end quotation

@node Models, Others, Lexical scoping, What are the differences between R and S?
@subsection Models

There are some differences in the modeling code, such as

@itemize @bullet
@item
Whereas in S, you would use @code{lm(y ~ x^3)} to regress @code{y} on
@code{x^3}, in R, you have to insulate powers of numeric vectors (using
@code{I()}), i.e., you have to use @code{lm(y ~ I(x^3))}.
@item
The glm family objects are implemented differently in R and S.  The same
functionality is available but the components have different names.
@item
Option @code{na.action} is set to @code{"na.omit"} by default in R,
but not set in S.
@item
Terms objects are stored differently.  In S a terms object is an
expression with attributes, in R it is a formula with attributes.  The
attributes have the same names but are mostly stored differently.
@item
Finally, in R @code{y ~ x + 0} is an alternative to @code{y ~ x - 1} for
specifying a model with no intercept.  Models with no parameters at all
can be specified by @code{y ~ 0}.
@end itemize

@node Others,  , Models, What are the differences between R and S?
@subsection  Others

Apart from lexical scoping and its implications, R follows the S
language definition in the Blue and White Books as much as possible, and
hence really is an ``implementation'' of S.  There are some intentional
differences where the behavior of S is considered ``not clean''.  In
general, the rationale is that R should help you detect programming
errors, while at the same time being as compatible as possible with S.

Some known differences are the following.

@itemize @bullet

@item
In R, if @code{x} is a list, then @code{x[i] <- NULL} and @code{x[[i]]
<- NULL} remove the specified elements from @code{x}.  The first of
these is incompatible with S, where it is a no-op.  (Note that you can
set elements to @code{NULL} using @code{x[i] <- list(NULL)}.)

@c @item
@c In R @code{x[-4]} fails if @code{x} is not @code{NULL} but has fewer
@c than 4 elements.  In S it has no effect.

@item
In S, the functions named @code{.First} and @code{.Last} in the
@file{.Data} directory can be used for customizing, as they are executed
at the very beginning and end of a session, respectively.

In R, the startup mechanism is as follows.  R first sources the system
startup file @file{@env{$@w{R_HOME}}/library/base/R/Rprofile}.  Then, it
searches for a site-wide startup profile unless the command line option
@option{--no-site-file} was given.  The name of this file is taken from
the value of the @env{R_PROFILE} environment variable.  If that variable
is unset, the default is @file{@env{$R_HOME}/etc/Rprofile.site}
(@file{@env{$R_HOME}/etc/Rprofile} in versions prior to 1.4.0).  This
code is loaded in package @strong{base}.  Then, unless
@option{--no-init-file} was given, R searches for a file called
@file{.Rprofile} in the current directory or in the user's home
directory (in that order) and sources it into the user workspace.  It
then loads a saved image of the user workspace from @file{.RData} in
case there is one (unless @option{--no-restore} was specified).  If
needed, the functions @code{.First()} and @code{.Last()} should be
defined in the appropriate startup profiles.

@item
In R, @code{T} and @code{F} are just variables being set to @code{TRUE}
and @code{FALSE}, respectively, but are not reserved words as in S and
hence can be overwritten by the user.  (This helps e.g.@: when you have
factors with levels @code{"T"} or @code{"F"}.)  Hence, when writing code
you should always use @code{TRUE} and @code{FALSE}.

@item
In R, @code{dyn.load()} can only load @emph{shared objects}, as created
for example by @kbd{R CMD SHLIB}.

@item
In R, @code{attach()} currently only works for lists and data frames,
but not for directories.  (In fact, @code{attach()} also works for R
data files created with @code{save()}, which is analogous to attaching
directories in S.)  Also, you cannot attach at position 1.

@item
Categories do not exist in R, and never will as they are deprecated now
in S.  Use factors instead.

@item
In R, @code{For()} loops are not necessary and hence not supported.

@item
In R, @code{assign()} uses the argument @option{envir=} rather than
@option{where=} as in S.

@item
The random number generators are different, and the seeds have different
length.

@item
R passes integer objects to C as @code{int *} rather than @code{long *}
as in S.

@item
R has no single precision storage mode.  However, as of version 0.65.1,
there is a single precision interface to C/@FORTRAN{} subroutines.

@item
By default, @code{ls()} returns the names of the objects in the current
(under R) and global (under S) environment, respectively.  For example,
given

@example
x <- 1; fun <- function() @{y <- 1; ls()@}
@end example

@noindent
then @code{fun()} returns @code{"y"} in R and @code{"x"} (together with
the rest of the global environment) in S.

@item
R allows for zero-extent matrices (and arrays, i.e., some elements of
the @code{dim} attribute vector can be 0).  This has been determined a
useful feature as it helps reducing the need for special-case tests for
empty subsets.  For example, if @code{x} is a matrix, @code{x[, FALSE]}
is not @code{NULL} but a ``matrix'' with 0 columns.  Hence, such objects
need to be tested for by checking whether their @code{length()} is zero
(which works in both R and S), and not using @code{is.null()}.

@item
Named vectors are considered vectors in R but not in S (e.g.,
@code{is.vector(c(a = 1:3))} returns @code{FALSE} in S and @code{TRUE}
in R).

@item
Data frames are not considered as matrices in R (i.e., if @code{DF} is a
data frame, then @code{is.matrix(DF)} returns @code{FALSE} in R and
@code{TRUE} in S).

@item
R by default uses treatment contrasts in the unordered case, whereas S
uses the Helmert ones.  This is a deliberate difference reflecting the
opinion that treatment contrasts are more natural.

@item
In R, the argument of a replacement function which corresponds to the
right hand side must be named @samp{value}.  E.g., @code{f(a) <- b} is
evaluated as @code{a <- "f<-"(a, value = b)}.  S always takes the last
argument, irrespective of its name.

@item
In S, @code{substitute()} searches for names for substitution in the
given expression in three places: the actual and the default arguments
of the matching call, and the local frame (in that order).  R looks in
the local frame only, with the special rule to use a ``promise'' if a
variable is not evaluated.  Since the local frame is initialized with
the actual arguments or the default expressions, this is usually
equivalent to S, until assignment takes place.

@item
In S, the index variable in a @code{for()} loop is local to the inside
of the loop.  In R it is local to the environment where the @code{for()}
statement is executed.

@item
In S, @code{tapply(simplify=TRUE)} returns a vector where R returns a
one-dimensional array (which can have named dimnames).

@item
In S(-@sc{Plus}) the C locale is used, whereas in R the current
operating system locale is used for determining which characters are
alphanumeric and how they are sorted.  This affects the set of valid
names for R objects (for example accented chars may be allowed in R) and
ordering in sorts and comparisons (such as whether @code{"aA" < "Bb"} is
true or false).  From version 1.2.0 the locale can be (re-)set in R by
the @code{Sys.setlocale()} function.

@item
In S, @code{missing(@var{arg})} remains @code{TRUE} if @var{arg} is
subsequently modified; in R it doesn't.

@item
From R version 1.3.0, @code{data.frame} strips @code{I()} when creating
(column) names.

@item
In R, the string @code{"NA"} is not treated as a missing value in a
character variable.  Use @code{as.character(NA)} to create a missing
character value.

@item
R disallows repeated formal arguments in function calls.

@item
In S, @code{dump()}, @code{dput()} and @code{deparse()} are essentially
different interfaces to the same code.  In R from version 2.0.0, this is
only true if the same @code{control} argument is used, but by default it
is not.  By default @code{dump()} tries to write code that will evaluate
to reproduce the object, whereas @code{dput()} and @code{deparse()}
default to options for producing deparsed code that is readable.

@item
In R, indexing a vector, matrix, array or data frame with @code{[} using
a character vector index looks only for exact matches (whereas @code{[[}
and @code{$} allow partial matches).  In S, @code{[} allows partial
matches.

@end itemize

There are also differences which are not intentional, and result from
missing or incorrect code in R.  The developers would appreciate hearing
about any deficiencies you may find (in a written report fully
documenting the difference as you see it).  Of course, it would be
useful if you were to implement the change yourself and make sure it
works.

@node Is there anything R can do that S-PLUS cannot?, What is R-plus?, What are the differences between R and S?, R and S
@section Is there anything R can do that @sc{S-Plus} cannot?

Since almost anything you can do in R has source code that you could
port to @SPLUS{} with little effort there will never be much you can do
in R that you couldn't do in @SPLUS{} if you wanted to.  (Note that
using lexical scoping may simplify matters considerably, though.)

R offers several graphics features that @SPLUS{} does not, such as finer
handling of line types, more convenient color handling (via palettes),
gamma correction for color, and, most importantly, mathematical
annotation in plot texts, via input expressions reminiscent of @TeX{}
constructs.  See the help page for @code{plotmath}, which features an
impressive on-line example.  More details can be found in Paul Murrell
and Ross Ihaka (2000), ``An Approach to Providing Mathematical
Annotation in Plots'', @url{http://www.amstat.org/publications/jcgs/, ,
@emph{Journal of Computational and Graphical Statistics}}, @strong{9},
582--599.

@node What is R-plus?,  , Is there anything R can do that S-PLUS cannot?, R and S
@section What is R-plus?

There is no such thing.

@node R Web Interfaces, R Add-On Packages, R and S, Top
@chapter R Web Interfaces

@strong{Rweb} is developed and maintained by
@email{jeff@@math.montana.edu, Jeff Banfield}.  The
@url{http://www.math.montana.edu/Rweb/, Rweb Home Page} provides access
to all three versions of Rweb---a simple text entry form that returns
output and graphs, a more sophisticated Javascript version that provides
a multiple window environment, and a set of point and click modules that
are useful for introductory statistics courses and require no knowledge
of the R language.  All of the Rweb versions can analyze Web accessible
datasets if a @acronym{URL} is provided.

The paper ``Rweb: Web-based Statistical Analysis'', providing a detailed
explanation of the different versions of Rweb and an overview of how
Rweb works, was published in the Journal of Statistical Software
(@url{http://www.stat.ucla.edu/journals/jss/v04/i01/}).

@email{ulfi@@cs.tu-berlin.de, Ulf Bartel} is working on
@strong{R-Online}, a simple on-line programming environment for R which
intends to make the first steps in statistical programming with R
(especially with time series) as easy as possible.  There is no need for
a local installation since the only requirement for the user is a
JavaScript capable browser.  See @url{http://osvisions.com/r-online/}
for more information.

@email{http://www.warwick.ac.uk/go/dfirth, David Firth} has written
@strong{CGIwithR}, an R add-on package available from @CRAN{}.  It
provides some simple extensions to R to facilitate running R scripts
through the CGI interface to a web server.  It is easily installed using
Apache under Linux and in principle should run on any platform that
supports R and a web server provided that the installer has the
necessary security permissions.

@strong{Rcgi} is a CGI WWW interface to R by @email{mjr@@dsl.pipex.com,
MJ Ray}.  It had the ability to use ``embedded code'': you could mix
user input and code, allowing the @HTML{} author to do anything from
load in data sets to enter most of the commands for users without
writing CGI scripts.  Graphical output was possible in PostScript or GIF
formats and the executed code was presented to the user for revision.
However, it is not clear if the project is still active.
@c URL no longer seems to work ...
@c See @url{http://stats.mth.uea.ac.uk/Rcgi/} for more information.
Currently, a modified version of @strong{Rcgi} by
@email{mai@@ms.uky.edu, Mai Zhou} (actually, two versions: one with
(bitmap) graphics and one without) as well as the original code are
available from @url{http://www.ms.uky.edu/~statweb/}.

@node R Add-On Packages, R and Emacs, R Web Interfaces, Top
@chapter R Add-On Packages

@menu
* Which add-on packages exist for R?::  
* How can add-on packages be installed?::  
* How can add-on packages be used?::  
* How can add-on packages be removed?::  
* How can I create an R package?::  
* How can I contribute to R?::  
@end menu

@node Which add-on packages exist for R?, How can add-on packages be installed?, R Add-On Packages, R Add-On Packages
@section Which add-on packages exist for R?

@menu
* Add-on packages in R::        
* Add-on packages from CRAN::   
* Add-on packages from Omegahat::  
* Add-on packages from Bioconductor::  
* Other add-on packages::       
@end menu

@node Add-on packages in R, Add-on packages from CRAN, Which add-on packages exist for R?, Which add-on packages exist for R?
@subsection Add-on packages in R

The R distribution comes with the following packages:

@table @strong
@c <FIXME>
@c 2.2.0
@item base
Base R functions (and datasets before R 2.0.0).
@item datasets
Base R datasets (added in R 2.0.0).
@item grDevices
Graphics devices for base and grid graphics (added in R 2.0.0).
@c </FIXME>
@item graphics
R functions for base graphics.
@item grid
A rewrite of the graphics layout capabilities, plus some support for
interaction.
@item methods
Formally defined methods and classes for R objects, plus other
programming tools, as described in the Green Book.
@item splines
Regression spline functions and classes.
@item stats
R statistical functions.
@item stats4
Statistical functions using S4 classes.
@item tcltk
Interface and language bindings to Tcl/Tk @acronym{GUI} elements.
@item tools
Tools for package development and administration.
@item utils
R utility functions.
@end table
These ``base packages'' were substantially reorganized in R 1.9.0.  The
former @strong{base} was split into the four packages @strong{base},
@strong{graphics}, @strong{stats}, and @strong{utils}.  Packages
@strong{ctest}, @strong{eda}, @strong{modreg}, @strong{mva},
@strong{nls}, @strong{stepfun} and @strong{ts} were merged into
@strong{stats}, package @strong{lqs} returned to the recommended package
@strong{MASS}, and package @strong{mle} moved to @strong{stats4}.

@node Add-on packages from CRAN, Add-on packages from Omegahat, Add-on packages in R, Which add-on packages exist for R?
@subsection Add-on packages from @acronym{CRAN}

The following packages are available from the @CRAN{} @file{src/contrib}
area.  (Packages denoted as @emph{Recommended} are to be included in all
binary distributions of R.)

@table @strong
@item AMORE
A MORE flexible neural network package, providing the TAO robust neural
network algorithm.
@item AlgDesign
Algorithmic experimental designs.  Calculates exact and approximate
theory experimental designs for D, A, and I criteria.
@item AnalyzeFMRI
Functions for I/O, visualisation and analysis of functional Magnetic
Resonance Imaging (fMRI) datasets stored in the ANALYZE format.
@item BHH2
Functions and data sets reproducing some examples in ``Statistics for
Experimenters II'' by G. E. P. Box, J. S. Hunter, and W. C. Hunter,
2005, John Wiley and Sons.
@item BMA
Bayesian Model Averaging for linear models, generalizable linear models
and survival models (Cox regression).
@item BRugs
OpenBUGS and its R interface BRugs.
@item Bhat
Functions for general likelihood exploration (MLE, MCMC, CIs).
@item Biodem
A number of functions for biodemographycal analysis.
@item Bolstad
Functions and data sets for the book ``Introduction to Bayesian
Statistics'' by W. M. Bolstad, 2004, John Wiley and Sons.
@item BradleyTerry
Specify and fit the Bradley-Terry model and structured versions.
@item BSDA
Data sets for the book ``Basic Statistics and Data Analysis'' by
L. J. Kitchens, 2003, Duxbury.
@item BsMD
Bayes screening and model discrimination follow-up designs.
@item CDNmoney
Components of Canadian monetary aggregates.
@item CGIwithR
Facilities for the use of R to write CGI scripts.
@item CircStats
Circular Statistics, from ``Topics in Circular Statistics'' by S. Rao
Jammalamadaka and A. SenGupta, 2001, World Scientific.
@item CoCo
Graphical modeling for contingency tables using CoCo.
@c <COMMENT>
@c Moved to the Archive on 2005-05-08 as requested by
@c Stephane Dray <dray@biomserv.univ-lyon1.fr>.
@c @item CoCoAn
@c Constrained Correspondence Analysis.
@c </COMMENT>
@item DAAG
Various data sets used in examples and exercises in ``Data Analysis and
Graphics Using R'' by John H. Maindonald and W. John Brown, 2003.
@item DBI
A common database interface (DBI) class and method definitions.  All
classes in this package are virtual and need to be extended by the
various DBMS implementations.
@item DCluster
A set of functions for the detection of spatial clusters of diseases
using count data.
@item Davies
Functions for the Davies quantile function and the Generalized Lambda
distribution.
@item Design
Regression modeling, testing, estimation, validation, graphics,
prediction, and typesetting by storing enhanced model design attributes
in the fit.  Design is a collection of about 180 functions that assist
and streamline modeling, especially for biostatistical and epidemiologic
applications.  It also contains new functions for binary and ordinal
logistic regression models and the Buckley-James multiple regression
model for right-censored responses, and implements penalized maximum
likelihood estimation for logistic and ordinary linear models.  Design
works with almost any regression model, but it was especially written to
work with logistic regression, Cox regression, accelerated failure time
models, ordinary linear models, and the Buckley-James model.
@item Devore5
Data sets and sample analyses from ``Probability and Statistics for
Engineering and the Sciences (5th ed)'' by Jay L. Devore, 2000, Duxbury.
@item Devore6
Data sets and sample analyses from ``Probability and Statistics for
Engineering and the Sciences (6th ed)'' by Jay L. Devore, 2003, Duxbury.
@item EMV
Estimation of missing values in a matrix by a @math{k}-th nearest
neighboors algorithm.
@item EbayesThresh
Empirical Bayes thresholding and related methods.
@item Ecdat
Data sets from econometrics textbooks.
@item Epi
Statistical analysis in epidemiology, with functions for demographic and
epidemiological analysis in the Lexis diagram.
@item Fahrmeir
Data from the book ``Multivariate Statistical Modelling Based
on Generalized Linear Models'' by Ludwig Fahrmeir and Gerhard Tutz
(1994),  Springer.
@item GDD
Platform and X11 independent device for creating bitmaps (png, gif and
jpeg) using the GD graphics library.
@item GPArotation
Gradient Projection Algorithm rotation for factor analysis.
@item GRASS
An interface between the GRASS geographical information system and R,
based on starting R from within the GRASS environment and chosen
LOCATION_NAME and MAPSET.  Wrapper and helper functions are provided for
a range of R functions to match the interface metadata structures.
@item GenKern
Functions for generating and manipulating generalised binned kernel
density estimates.
@item GeneNT
Relevance or Dependency network and signaling pathway discovery.
@item GeneTS
A package for analysing multiple gene expression time series data.
Currently, implements methods for cell cycle analysis and for inferring
large sparse graphical Gaussian models.
@item Geneland
MCMC inference from individual genetic data based on a spatial
statistical model.
@item HI
Simulation from distributions supported by nested hyperplanes.
@item HTMLapplets
Functions inserting dynamic scatterplots and grids in documents
generated by @strong{R2HTML}. 
@item HighProbability
Estimation of the alternative hypotheses having frequentist or Bayesian
probabilities at least as great as a specified threshold, given a list
of @math{p}-values.
@item Hmisc
Functions useful for data analysis, high-level graphics, utility
operations, functions for computing sample size and power, importing
datasets, imputing missing values, advanced table making, variable
clustering, character string manipulation, conversion of S objects to
@LaTeX{} code, recoding variables, and bootstrap repeated measures
analysis.
@item HyperbolicDist
Basic functions for the hyperbolic distribution: probability density
function, distribution function, quantile function, a routine for
generating observations from the hyperbolic, and a function for fitting
the hyperbolic distribution to data.
@item IDPmisc
Utilities from the Institute of Data Analyses and Process Design,
IDP/ZHW.
@item ISwR
Data sets for ``Introductory Statistics with R'' by Peter Dalgaard,
2002, Springer.
@item Icens
Functions for computing the NPMLE for censored and truncated data.
@item KMsurv
Data sets and functions for ``Survival Analysis, Techniques for Censored
and Truncated Data'' by Klein and Moeschberger, 1997, Springer.
@item KernSmooth
Functions for kernel smoothing (and density estimation) corresponding to
the book ``Kernel Smoothing'' by M. P. Wand and M. C. Jones, 1995.
@emph{Recommended}.
@item LDheatmap
Heat maps of linkage disequilibrium measures.
@item LMGene
Date transformation and identification of differentially expressed genes
in gene expression arrays.
@item LogicReg
Routines for Logic Regression.
@item MASS
Functions and datasets from the main package of Venables and Ripley,
``Modern Applied Statistics with S''.  Contained in the @file{VR}
bundle.  @emph{Recommended}.
@item MCMCpack
Markov chain Monte Carlo (MCMC) package: functions for posterior
simulation for a number of statistical models.
@item MEMSS
Data sets and sample analyses from ``Mixed-effects Models in S and
S-PLUS'' by J. Pinheiro and D. Bates, 2000, Springer.
@item MNP
Fitting Bayesian Multinomial Probit models via Markov chain Monte Carlo.
Along with the standard Multinomial Probit model, it can also fit models
with different choice sets for each observation and complete or partial
ordering of all the available alternatives.
@item MPV
Data sets from the book ``Introduction to Linear Regression Analysis''
by D. C. Montgomery, E. A. Peck, and C. G. Vining, 2001, John Wiley and
Sons.
@item Malmig
An implementation of Malecot migration model together with a number of
related functions.
@item MarkedPointProcess
Non-parametric analysis of the marks of marked point processes.
@item MatchIt
Select matched samples of the original treated and control groups with
similar covariate distributions.
@item Matching
Multivariate and propensity score matching with formal tests of balance.
@item Matrix
A Matrix package.
@item NADA
Methods described in ``Nondetects And Data Analysis: Statistics for
Censored Environmental Data'' by Dennis R. Helsel, 2004, John Wiley and
Sons.
@item NISTnls
A set of test nonlinear least squares examples from @acronym{NIST}, the
U.S. National Institute for Standards and Technology.
@item NORMT3
Evaluates complex erf, erfc and density of sum of Gaussian and Student's 
@math{t}.
@item Oarray
Arrays with arbitrary offsets.
@item PBSmapping
Software evolved from fisheries research conducted at the Pacific
Biological Station (PBS) in Nanaimo, British Columbia, Canada.  Draws
maps and implements other GIS procedures.
@item PHYLOGR
Manipulation and analysis of phylogenetically simulated data sets (as
obtained from PDSIMUL in package PDAP) and phylogenetically-based
analyses using GLS.
@item PK
Estimation of pharmacokinetic parameters.
@item PTAk
A multiway method to decompose a tensor (array) of any order, as a
generalisation of SVD also supporting non-identity metrics and
penalisations.  Also includes some other multiway methods.
@item ProbForecastGOP
Probabilistic weather field forecasts using the Geostatistical Output
Perturbation method introduced by Gel, Raftery and Gneiting (2004).
@item R.matlab
Read and write of MAT files together with R-to-Matlab connectivity.
@item R.oo
R object-oriented programming with or without references.
@item R2HTML
Functions for exporting R objects & graphics in an @HTML{} document.
@item R2WinBUGS
Running WinBUGS from R: call a BUGS model, summarize inferences and
convergence in a table and graph, and save the simulations in arrays for
easy access in R.
@item RArcInfo
Functions to import Arc/Info V7.x coverages and data.
@item RColorBrewer
ColorBrewer palettes for drawing nice maps shaded according to a
variable.
@item RGrace
Mouse/menu driven interactive plotting application.
@item RGraphics
Data and functions from the book ``R Graphics'' by Paul Murrell, 2005,
Chapman & Hall/CRC.
@item RII
Estimation of the relative index of inequality for interval-censored
data using natural cubic splines.
@item RMySQL
An interface between R and the MySQL database system.
@item RNetCDF
An interface to Unidata's NetCDF library functions (version 3) and
furthermore access to Unidata's udunits calendar conversions.
@item ROCR
Visualizing the performance of scoring classifiers.
@item RODBC
An @acronym{ODBC} database interface.
@item ROracle
Oracle Database Interface driver for R.  Uses the ProC/C++ embedded SQL.
@item RQuantLib
Provides access to (some) of the QuantLib functions from within R;
currently limited to some Option pricing and analysis functions.  The
QuantLib project aims to provide a comprehensive software framework for
quantitative finance.
@item RSQLite
Database Interface R driver for SQLite.  Embeds the SQLite database
engine in R.
@item RScaLAPACK
An interface to ScaLAPACK functions from R.
@item RSvgDevice
A graphics device for R that uses the new w3.org @XML{} standard for
Scalable Vector Graphics.
@item RUnit
Functions implementing a standard Unit Testing framework, with
additional code inspection and report generation tools.
@item RWinEdt
A plug in for using WinEdt as an editor for R.
@item RadioSonde
A collection of programs for reading and plotting SKEW-T,log p diagrams
and wind profiles for data collected by radiosondes (the typical weather
balloon-borne instrument).
@item RandomFields
Creating random fields using various methods.
@item Rcmdr
A platform-independent basic-statistics GUI (graphical user interface)
for R, based on the @strong{tcltk} package.
@item ResistorArray
Electrical properties of resistor networks.
@item Rfwdmv
Forward Search for Multivariate Data.
@item Rlab
Functions and data sets for the NCSU ST370 class.
@c <COMMENT>
@c Moved to the Archive on 2004-08-11.
@c @item RmSQL
@c An interface between R and the mSQL database system.
@c </COMMENT>
@item Rmpi
An interface (wrapper) to MPI (Message-Passing Interface) APIs.  It also
provides an interactive R slave environment in which distributed
statistical computing can be carried out.
@c @item Rnotes
@c The data sets for the exercises in ``An Introduction to R''
@c (@pxref{What documentation exists for R?}).
@item Rpad
Utility functions for the Rpad workbook-style interface.
@c REMOVED 2001-12-08
@c @item Rstreams
@c Binary file stream support functions.
@item Rwave
An environment for the time-frequency analysis of 1-D signals (and
especially for the wavelet and Gabor transforms of noisy signals), based
on the book ``Practical Time-Frequency Analysis: Gabor and Wavelet
Transforms with an Implementation in S'' by Rene Carmona, Wen L. Hwang
and Bruno Torresani, 1998, Academic Press.
@item SASmixed
Data sets and sample linear mixed effects analyses corresponding to the
examples in ``SAS System for Mixed Models'' by R. C. Littell,
G. A. Milliken, W. W. Stroup and R. D. Wolfinger, 1996, SAS Institute.
@item SIN
A SINful approach to selection of Gaussian Graphical Markov Models.
@item SciViews
A bundle of packages to implement a full reusable GUI API for R.
Contains @strong{svGUI} with the main GUI features, @strong{svDialogs}
for the dialog boxes, @strong{svIO} for data import/export,
@strong{svMisc} with miscellaneous supporting functions, and
@strong{svViews} providing views and report features (views are
@acronym{HTML} presentations of the content of R objects, combining
text, tables and graphs in the same document).
@item SemiPar
Functions for semiparametric regression analysis, to complement the
book ``Semiparametric Regression'' by R. Ruppert, M. P. Wand, and
R. J. Carroll, 2003, Cambridge University Press.
@item SenSrivastava
Collection of datasets from ``Regression Analysis, Theory, Methods and
Applications'' by A. Sen and M. Srivastava, 1990, Springer.
@item SeqKnn
Sequential KNN imputation.
@item SharedHT2
Shared Hotelling @math{T^2} test for small sample microarray experiments.
@item SoPhy
Soil Physics Tools: simulation of water flux and solute transport in
soil.
@item SparseLogReg
Sparse logistic regression.
@item SparseM
Basic linear algebra for sparse matrices.
@item StatDataML
Read and write StatDataML.
@item SuppDists
Ten distributions supplementing those built into R (Inverse Gauss,
Kruskal-Wallis, Kendall's Tau, Friedman's chi squared, Spearman's rho,
maximum F ratio, the Pearson product moment correlation coefficiant,
Johnson distributions, normal scores and generalized hypergeometric
distributions).
@item UNF
Tools for creating universal numeric fingerprints for data.
@item UsingR
Data sets to accompany the textbook ``Using R for Introductory
Statistics'' by J. Verzani, 2005, Chapman & Hall/CRC.
@item VLMC
Functions, classes & methods for estimation, prediction, and simulation
(bootstrap) of VLMC (Variable Length Markov Chain) models.
@item VaR
Methods for calculation of Value at Risk (VaR).
@item XML
Tools for reading @XML{} documents and DTDs.
@item Zelig
Everyone's statistical software: an easy-to-use program that can
estimate, and help interpret the results of, an enormous range of
statistical models.
@item abind
Combine multi-dimensional arrays.
@item accuracy
A suite of tools designed to test and improve the accuracy of
statistical computation.
@item acepack
ACE (Alternating Conditional Expectations) and AVAS (Additivity and
VAriance Stabilization for regression) methods for selecting regression
transformations.
@item adapt
Adaptive quadrature in up to 20 dimensions.
@item ade4
Multivariate data analysis and graphical display.
@item adehabitat
A collection of tools for the analysis of habitat selection by animals.
@item adlift
Adaptive Wavelet transforms for signal denoising.
@item agce
Analysis of growth curve experiments.
@item akima
Linear or cubic spline interpolation for irregularly gridded data.
@item alr3
Methods and data to accompany the textbook ``Applied Linear Regression''
by S. Weisberg, 2005, Wiley.
@item amap
Another Multidimensional Analysis Package.
@item anm
Analog model for statistical/empirical downscaling.
@item aod
Analysis of Overdispersed Data.
@item ape
Analyses of Phylogenetics and Evolution, providing functions for reading
and plotting phylogenetic trees in parenthetic format (standard Newick
format), analyses of comparative data in a phylogenetic framework,
analyses of diversification and macroevolution, computing distances from
allelic and nucleotide data, reading nucleotide sequences from GenBank
via internet, and several tools such as Mantel's test, computation of
minimum spanning tree, or the population parameter theta based on
various approaches.
@item arules
Mining association rules and frequent itemsets with R.
@item ash
David Scott's ASH routines for 1D and 2D density estimation.
@item assist
A suite of functions implementing smoothing splines. 
@item asypow
A set of routines that calculate power and related quantities utilizing
asymptotic likelihood ratio methods.
@item aws
Functions to perform adaptive weights smoothing.
@item bayesSurv
Bayesian survival regression with flexible error and (later on also
random effects) distributions.
@item bayesm
Bayes Inference for Marketing/Micro-econometrics.
@item bayesmix
Bayesian mixture models of univariate Gaussian distributions using JAGS.
@item baymvb
Bayesian analysis of multivariate binary data.
@item betareg
Beta regression for modeling rates and proportions.
@item bicreduc
Reduction algorithm for the NPMLE for the distribution function of
bivariate interval-censored data.
@item bim
Bayesian interval mapping diagnostics:  functions to interpret QTLCart
and Bmapqtl samples.
@item bindata
Generation of correlated artificial binary data.
@item biopara
Self-contained parallel system for R.
@item bitops
Functions for Bitwise operations on integer vectors.
@item blighty
Function for drawing the coastline of the United Kingdom.
@item boa
Bayesian Output Analysis Program for MCMC.
@item boolean
Boolean logit and probit: a procedure for testing Boolean hypotheses.
@item boost
Boosting methods for real and simulated data, featuring `BagBoost',
`LogitBoost', `AdaBoost', and `L2Boost'.
@item boot
Functions and datasets for bootstrapping from the book ``Bootstrap
Methods and Their Applications'' by A. C. Davison and D. V. Hinkley,
1997, Cambridge University Press.  @emph{Recommended}.
@item bootstrap
Software (bootstrap, cross-validation, jackknife), data and errata for
the book ``An Introduction to the Bootstrap'' by B. Efron and
R. Tibshirani, 1993, Chapman and Hall.
@item bqtl
QTL mapping toolkit for inbred crosses and recombinant inbred lines.
Includes maximum likelihood and Bayesian tools.
@item brlr
Bias-reduced logistic regression: fits logistic regression models by
maximum penalized likelihood.
@item caMassClass
Processing and Classification of protein mass spectra (SELDI) data.
@item caTools
Miscellaneous utility functions, including reading/writing ENVI binary
files, a LogitBoost classifier, and a base64 encoder/decoder.
@item car
Companion to Applied Regression, containing functions for applied
regession, linear models, and generalized linear models, with an
emphasis on regression diagnostics, particularly graphical diagnostic
methods.
@item cat
Analysis of categorical-variable datasets with missing values.
@item catspec
Special models for categorical variables.
@item cclust
Convex clustering methods, including @math{k}-means algorithm, on-line
update algorithm (Hard Competitive Learning) and Neural Gas algorithm
(Soft Competitive Learning) and calculation of several indexes for
finding the number of clusters in a data set.
@item cfa
Analysis of configuration frequencies.
@item chplot
Augmented convex hull plots: informative and nice plots for grouped
bivariate data.
@item changeLOS
Change in length of hospital stay (LOS).
@item chron
A package for working with chronological objects (times and dates).
@item circular
Circular statistics, from ``Topics in Circular Statistics'' by Rao
Jammalamadaka and A. SenGupta, 2001, World Scientific.
@item clac
Clust Along Chromosomes, a method to call gains/losses in CGH array
data.
@item class
Functions for classification (@math{k}-nearest neighbor and LVQ).
Contained in the @file{VR} bundle.  @emph{Recommended}.
@item classPP
Projection Pursuit for supervised classification.
@item clim.pact
Climate analysis and downscaling for monthly and daily data.
@item climatol
Functions to fill missing data in climatological (monthly) series and to
test their homogeneity, plus functions to draw wind-rose and
Walter&Lieth diagrams. 
@item clines
Calculates Contour Lines.
@item clue
CLUster Ensembles.
@item cluster
Functions for cluster analysis.  @emph{Recommended}.
@item cmprsk
Estimation, testing and regression modeling of subdistribution functions
in competing risks.
@item cobs
Constrained B-splines: qualitatively constrained (regression) smoothing
via linear programming.
@item coda
Output analysis and diagnostics for Markov Chain Monte Carlo (MCMC)
simulations.
@item coin
COnditional INference procedures for the general independence problem
including two-sample, @math{K}-sample, correlation, censored, ordered
and multivariate problems.
@item colorspace
Mapping between assorted color spaces.
@item combinat
Combinatorics utilities.
@item compositions
Functions for the consistent analysis of compositional data (e.g.,
portions of substances) and positive numbers (e.g., concentrations).
@item concor
Concordance, providing ``SVD by blocks''.
@item concord
Measures of concordance and reliability.
@item conf.design
A series of simple tools for constructing and manipulating confounded
and fractional factorial designs.
@item covRobust
Robust covariance estimation via nearest neighbor cleaning.
@item cramer
Routine for the multivariate nonparametric Cramer test.
@item crossdes
Functions for the construction and randomization of balanced carryover
balanced designs, to check given designs for balance, and for simulation
studies on the validity of two randomization procedures.
@item ctv
Server-side and client-side tools for @acronym{CRAN} task views.
@item cyclones
Cyclone identification.
@item date
Functions for dealing with dates.  The most useful of them accepts a
vector of input dates in any of the forms @samp{8/30/53},
@samp{30Aug53}, @samp{30 August 1953}, @dots{}, @samp{August 30 53}, or
any mixture of these.
@item dblcens
Calculates the NPMLE of the survival distribution for doubly censored
data.
@item deal
Bayesian networks with continuous and/or discrete variables can be
learned and compared from data.
@item debug
Debugger for R functions, with code display, graceful error recovery,
line-numbered conditional breakpoints, access to exit code, flow
control, and full keyboard input.
@item deldir
Calculates the  Delaunay triangulation and the Dirichlet or Voronoi
tesselation (with respect to the entire plane) of a planar point set.
@item depmix
Dependent Mixture Models: fit (multi-group) mixtures of latent Markov
models on mixed categorical and continuous (time series) data.
@item diamonds
Functions for illustrating aperture-4 diamond partitions in the plane,
or on the surface of an octahedron or icosahedron, for use as analysis
or sampling grids.
@item dichromat
Color schemes for dichromats: collapse red-green distinctions to
simulate the effects of colour-blindness.
@item digest
Two functions for the creation of ``hash'' digests of arbitrary R
objects using the md5 and sha-1 algorithms permitting easy comparison of
R language objects.
@item diptest
Compute Hartigan's dip test statistic for unimodality.
@item dispmod
Functions for modelling dispersion in GLMs.
@item distr
An object orientated implementation of distributions and some additional
functionality.
@item dr
Functions, methods, and datasets for fitting dimension reduction
regression, including pHd and inverse regression methods SIR and SAVE.
@item drfit
Dose-response data evaluation.
@item dse
Dynamic System Estimation, a multivariate time series package bundle.
Contains @strong{dse1} (the base system, including multivariate ARMA and
state space models), @strong{dse2} (extensions for evaluating estimation
techniques, forecasting, and for evaluating forecasting model),
@strong{tframe} (functions for writing code that is independent of the
representation of time), and @strong{setRNG} (a mechanism for generating
the same random numbers in S and R).
@item dyn
Time series regression.
@item dynamicGraph
Interactive graphical tool for manipulating graphs.
@item dynlm
Dynamic linear models and time series regression.
@item e1071
Miscellaneous functions used at the Department of Statistics at TU Wien
(E1071), including moments, short-time Fourier transforms, Independent
Component Analysis, Latent Class Analysis, support vector machines, and
fuzzy clustering, shortest path computation, bagged clustering, and some
more.
@item eba
Fitting and testing probabilistic choice models, especially the BTL,
elimination-by-aspects (EBA), and preference tree (Pretree) models.
@item eco
Fitting Bayesian models of ecological inference in 2 by 2 tables.
@item edci
Edge Detection and Clustering in Images.
@item effects
Graphical and tabular effect displays, e.g., of interactions, for linear
and generalised linear models.
@item eha
A package for survival and event history analysis.
@item elasticnet
Elastic net regularization and variable selection.
@item ellipse
Package for drawing ellipses and ellipse-like confidence regions.
@item emme2
Functions to read from and write to an EMME/2 databank.
@item emplik
Empirical likelihood ratio for means/quantiles/hazards from possibly
right censored data.
@item energy
E-statistics (energy) tests for comparing distributions: multivariate
normality, Poisson test, multivariate @math{k}-sample test for equal
distributions, hierarchical clustering by e-distances.
@item ensembleBMA
Probabilistic forecasting using Bayesian Model Averaging of ensembles
using a mixture of normal distributions.
@item epitools
Basic tools for applied epidemiology.
@item epsi
Edge Preserving Smoothing for Images.
@item evd
Functions for extreme value distributions.  Extends simulation,
distribution, quantile and density functions to univariate, bivariate
and (for simulation) multivariate parametric extreme value
distributions, and provides fitting functions which calculate maximum
likelihood estimates for univariate and bivariate models.
@item evdbayes
Functions for the bayesian analysis of extreme value models, using MCMC
methods.
@item evir
Extreme Values in R: Functions for extreme value theory, which may be
divided into the following groups; exploratory data analysis, block
maxima, peaks over thresholds (univariate and bivariate), point
processes, gev/gpd distributions.
@item exactLoglinTest
Monte Carlo exact tests for log-linear models.
@item exactRankTests
Computes exact @math{p}-values and quantiles using an implementation of
the Streitberg/Roehmel shift algorithm.
@item fBasics
The Rmetrics module for ``Markets, basic statistics, and stylized
facts''.  Rmetrics is an environment and software collection for
teaching financial engineering and computational finance
(@url{http://www.Rmetrics.org/}).
@item fCalendar
The Rmetrics module for ``Date, Time and Calendars''.
@item fExtremes
The Rmetrics module for ``Beyond the Sample, Dealing with Extreme
Values''.
@item fMultivar
The Rmetrics module for ``Multivariate Data Analysis''.
@item fOptions
The Rmetrics module for ``The Valuation of Options''.
@item fPortfolio
The Rmetrics module for ``Pricing and Hedging of Options''.
@item fSeries
The Rmetrics module for ``The Dynamical Process Behind Financial
Markets''.
@item far
Modelization for Functional AutoRegressive processes.
@item faraway
Functions and datasets for books by Julian Faraway.
@item fastICA
Implementation of FastICA algorithm to perform Independent Component
Analysis (ICA) and Projection Pursuit.
@item fda
Functional Data Analysis: analysis of data where the basic observation
is a function of some sort.
@item fdim
Functions for calculating fractal dimension.
@item fields
A collection of programs for curve and function fitting with an emphasis
on spatial data.  The major methods implemented include cubic and thin
plate splines, universal Kriging and Kriging for large data sets.  The
main feature is that any covariance function implemented in R can be
used for spatial prediction.
@item flexmix
Flexible Mixture Modeling: a general framework for finite mixtures of
regression models using the EM algorithm.
@item foreign
Functions for reading and writing data stored by statistical software
like Minitab, S, SAS, SPSS, Stata, Systat, etc.  @emph{Recommended}.
@item fork
Functions for handling multiple processes: simple wrappers around the
Unix process management API calls.
@item fortunes
R fortunes.
@item forward
Forward search approach to robust analysis in linear and generalized
linear regression models.
@item fpc
Fixed point clusters, clusterwise regression and discriminant plots.
@item fracdiff
Maximum likelihood estimation of the parameters of a fractionally
differenced ARIMA(@math{p,d,q}) model (Haslett and Raftery, Applied
Statistics, 1989).
@item frailtypack
Fit a shared gamma frailty model and Cox proportional hazards model
using a Penalized Likelihood on the hazard function.
@item ftnonpar
Features and strings for nonparametric regression.
@item g.data
Create and maintain delayed-data packages (DDP's).
@item gRbase
A package for graphical modelling in R.  Defines S4 classes for
graphical meta data and graphical models, and illustrates how
hierarchical log-linear models may be implemented and combined with
@strong{dynamicGraph}.
@item gafit
Genetic algorithm for curve fitting.
@item gam
Functions for fitting and working with Generalized Additive Models, as
described in chapter 7 of the White Book, and in ``Generalized Additive
Models'' by T. Hastie and R. Tibshirani (1990).
@item gap
Genetic analysis package for both population and family data.  Includes
@strong{pathmix} for path analysis of pairs of relatives, and
@strong{pointer} for complex segregation analysis.
@item gbm
Generalized Boosted Regression Models: implements extensions to Freund
and Schapire's AdaBoost algorithm and J. Friedman's gradient boosting
machine.  Includes regression methods for least squares, absolute loss,
logistic, Poisson, Cox proportional hazards partial likelihood, and
AdaBoost exponential loss.
@item gclus
Clustering Graphics.  Orders panels in scatterplot matrices and parallel
coordinate displays by some merit index.
@item gcmrec
Parameters estimation of the general semiparametric model for recurrent
event data proposed by Pe@~na and Hollander.
@item gdata
Various functions to manipulate data.
@item gee
An implementation of the Liang/Zeger generalized estimating equation
approach to GLMs for dependent data.
@item geepack
Generalized estimating equations solver for parameters in mean, scale,
and correlation structures, through mean link, scale link, and
correlation link.  Can also handle clustered categorical responses.
@item genalg
R based genetic algorithm for binary and floating point chromosomes.
@item genetics
Classes and methods for handling genetic data.  Includes classes to
represent genotypes and haplotypes at single markers up to multiple
markers on multiple chromosomes, and functions for allele frequencies,
flagging homo/heterozygotes, flagging carriers of certain alleles,
computing disequlibrium, testing Hardy-Weinberg equilibrium, @dots{}
@item geoR
Functions to perform geostatistical data analysis including model-based
methods.
@item geoRglm
Functions for inference in generalised linear spatial models.
@item geometry
Mesh generation and surface tesselation, based on the Qhull library.
@item ggm
Functions for defining directed acyclic graphs and undirected graphs,
finding induced graphs and fitting Gaussian Markov models.
@item gld
Basic functions for the generalised (Tukey) lambda distribution.
@item gllm
Routines for log-linear models of incomplete contingency tables,
including some latent class models via EM and Fisher scoring approaches.
@item glmmML
A Maximum Likelihood approach to generalized linear models with random
intercept.
@item glpk
Interface to the @acronym{GNU} Linear Programming Kit (GLPK).
@item gmodels
Various functions to manipulate models.
@item gmp
Arithmetic ``without limitations'' using the @acronym{GNU} Multiple
Precision library.
@item gpclib
General polygon clipping routines for R based on Alan Murta's C
library.
@item gpls
Classification using generalized partial least squares for two-group and
multi-group (more than 2 group) classification.
@item gplots
Various functions to draw plots.
@item grasper
Generalized Regression Analysis and Spatial Predictions for R.
@item gregmisc
Miscellaneous functions written/maintained by Gregory R. Warnes.
@item gridBase
Integration of base and grid graphics.
@item gsl
Wrapper for special functions of the Gnu Scientific Library (GSL).
@item gss
A comprehensive package for structural multivariate function estimation
using smoothing splines.
@item gstat
multivariable geostatistical modelling, prediction and simulation.
Includes code for variogram modelling; simple, ordinary and universal
point or block (co)kriging, sequential Gaussian or indicator
(co)simulation, and map plotting functions.
@item gtkDevice
GTK graphics device driver that may be used independently of the R-GNOME
interface and can be used to create R devices as embedded components in
a GUI using a Gtk drawing area widget, e.g., using RGtk.
@item gtools
Various functions to help manipulate data.
@item hapassoc
Likelihood inference of trait associations with SNP haplotypes and other 
attributes using the EM Algorithm.
@item haplo.score
Score tests for association of traits with haplotypes when linkage phase
is ambiguous.
@item haplo.stats
Statistical analysis of haplotypes with traits and covariates when
linkage phase is ambiguous.
@item hdf5
Interface to the @acronym{NCSA} HDF5 library.
@item hett
Functions for the fitting and summarizing of heteroscedastic
t-regression.
@item hier.part
Hierarchical Partitioning: variance partition of a multivariate data
set.
@item hierfstat
Estimation of hierarchical F-statistics from haploid or diploid genetic
data with any numbers of levels in the hierarchy, and tests for the
significance of each F and variance components.
@item hmm.discnp
Hidden Markov models with discrete non-parametric observation
distributions.
@item homals
Homogeneity Analysis (HOMALS) package with optional Tcl/Tk interface.
@item hopach
Hierarchical Ordered Partitioning and Collapsing Hybrid (HOPACH).
@item httpRequest
Implements HTTP Request protocols (GET, POST, and multipart POST
requests).
@item hwde
Models and tests for departure from Hardy-Weinberg equilibrium and
independence between loci.
@item ifs
Iterated Function Systems distribution function estimator.
@item impute
Imputation for microarray data (currently KNN only).
@item ineq
Inequality, concentration and poverty measures, and Lorenz curves
(empirical and theoretic).
@item intcox
Implementation of the Iterated Convex Minorant Algorithm for the Cox
proportional hazard model for interval censored event data.
@c  @item integrate
@c  Adaptive quadrature in up to 20 dimensions.
@item ipred
Improved predictive models by direct and indirect bootstrap aggregation
in classification and regression as well as resampling based estimators
of prediction error.
@item irr
Coefficients of Interrater Reliability and Agreement for quantitative,
ordinal and nominal data.
@item ismev
Functions to support the computations carried out in ``An Introduction
to Statistical Modeling of Extreme Values;' by S. Coles, 2001, Springer.
The functions may be divided into the following groups; maxima/minima,
order statistics, peaks over thresholds and point processes.
@item its
An S4 class for handling irregular time series.
@item kernlab
Kernel-based machine learning methods including support vector machines.
@item kinship
Mixed-effects Cox models, sparse matrices, and modeling data from large
pedigrees.
@item kknn
Weighted @math{k}-nearest neighbors classification and regression.
@item klaR
Miscellaneous functions for classification and visualization developed
at the Department of Statistics, University of Dortmund.
@item knnTree
Construct or predict with @math{k}-nearest-neighbor classifiers, using
cross-validation to select @math{k}, choose variables (by forward or
backwards selection), and choose scaling (from among no scaling, scaling
each column by its SD, or scaling each column by its MAD).  The finished
classifier will consist of a classification tree with one such
@math{k}-nn classifier in each leaf.
@item knncat
Nearest-neighbor classification with categorical variables.
@item ks
Kernel smoothing: bandwidth matrices for kernel density estimators and
kernel discriminant analysis for bivariate data.
@item kza
Kolmogorov-Zurbenko Adpative filter for locating change points in a time
series.
@item labdsv
Laboratory for Dynamic Synthetic Vegephenomenology.
@item labstatR
Functions for the book ``Laboratorio di statistica con R'' by
S. M. Iacus and G. Masarotto, 2002, McGraw-Hill.  Function names and
documentation in Italian.
@item lars
Least Angle Regression, Lasso and Forward Stagewise: efficient
procedures for fitting an entire lasso sequence with the cost of a
single least squares fit.
@item lasso2
Routines and documentation for solving regression problems while
imposing an L1 constraint on the estimates, based on the algorithm of
Osborne et al.@: (1998).
@item lattice
Lattice graphics, an implementation of Trellis Graphics functions.
@emph{Recommended}.
@item latticeExtra
Generic functions and standard methods for Trellis-based displays.
@item lazy
Lazy learning for local regression.
@item ldDesign
Design of experiments for detection of linkage disequilibrium,
@item leaps
A package which performs an exhaustive search for the best subsets of a
given set of potential regressors, using a branch-and-bound algorithm,
and also performs searches using a number of less time-consuming
techniques.
@item lgtdl
A set of methods for longitudinal data objects.
@item limma
LInear Models for MicroArray data.
@item linprog
Solve linear programming/linear optimization problems by using the
simplex algorithm.
@item lme4
Fit linear and generalized linear mixed-effects models.
@item lmeSplines
Fit smoothing spline terms in Gaussian linear and nonlinear
mixed-effects models.
@item lmm
Linear mixed models.
@item lmtest
A collection of tests on the assumptions of linear regression models
from the book ``The linear regression model under test'' by W. Kraemer
and H. Sonnberger, 1986, Physica.
@item locfdr
Computation of local false discovery rates.
@item locfit
Local Regression, likelihood and density estimation.
@item logistf
Firth's bias reduced logistic regression approach with penalized profile
likelihood based confidence intervals for parameter estimates.
@item logspline
Logspline density estimation.
@item lokern
Kernel regression smoothing with adaptive local or global plug-in
bandwidth selection.
@item lpSolve
Functions that solve general linear/integer problems, assignment
problems, and transportation problems via interfacing Lp_solve.
@item lpridge
Local polynomial (ridge) regression.
@item ltm
Analysis of multivariate Bernoulli data using latent trait models
(including the Rasch model) under the Item Response Theory approach.
@item mAr
Estimation of multivariate AR models through a computationally efficient
stepwise least-squares algorithm.
@item maanova
Analysis of N-dye Micro Array experiments using mixed model effect.
Contains anlysis of variance, permutation and bootstrap, cluster and
consensus tree.
@item magic
A variety of methods for creating magic squares of any order greater
than 2, and various magic hypercubes.
@item mapdata
Supplement to package @strong{maps}, providing the larger and/or
higher-resolution databases.
@item mapproj
Map Projections: converts latitude/longitude into projected coordinates.
@item maps
Draw geographical maps.  Projection code and larger maps are in separate
packages.
@item maptools
Set of tools for manipulating and reading geographic data, in particular
ESRI shapefiles.
@item maptree
Functions with example data for graphing and mapping models from
hierarchical clustering and classification and regression trees.
@c <COMMENT>
@c Moved to the Archive on 2005-04-29.
@c @item mathgraph
@c Tools for constructing and manipulating objects from a class of directed
@c and undirected graphs.
@c </COMMENT>
@item matlab
Emulate MATLAB code using R.
@item maxstat
Maximally selected rank and Gauss statistics with several p-value
approximations.
@item mcgibbsit
Warnes and Raftery's MCGibbsit MCMC diagnostic.
@item mclust
Model-based cluster analysis: the 2002 version of MCLUST.
@c <COMMENT>
@c Moved to Archive on 2004-02-19 as requested by the maintainer.
@c @item mclust1998
@c Model-based cluster analysis: the 1998 version of MCLUST.
@c </COMMENT>
@item mcmc
Functions for Markov Chain Monte Carlo (MCMC).
@item mda
Code for mixture discriminant analysis (MDA), flexible discriminant
analysis (FDA), penalized discriminant analysis (PDA), multivariate
additive regression splines (MARS), adaptive back-fitting splines
(BRUTO), and penalized regression.
@item meanscore
Mean Score method for missing covariate data in logistic regression
models.
@item merror
Accuracy and precision of measurements.
@item meta
Fixed and random effects meta-analysis, with functions for tests of
bias, forest and funnel plot.
@item mfp
Multiple Fractional Polynomials.
@item mgcv
Routines for GAMs and other genralized ridge regression problems with
multiple smoothing parameter selection by GCV or UBRE.
@emph{Recommended}.
@item micEcon
Tools for microeconomic analysis and microeconomic modelling.
@item mimR
An R interface to MIM for graphical modeling in R.
@item minpack.lm
R interface for two functions from the MINPACK least squares
optimization library, solving the nonlinear least squares problem by a
modification of the Levenberg-Marquardt algorithm.
@item misc3d
A collection of miscellaneous 3d plots, including rgl-based isosurfaces.
@item mitools
Tools to perform analyses and combine results from multiple-imputation
datasets.
@item mix
Estimation/multiple imputation programs for mixed categorical and
continuous data.
@item mixreg
Functions to fit mixtures of regressions.
@item mlbench
A collection of artificial and real-world machine learning benchmark
problems, including the Boston housing data.
@item mlica
Independent Component Analysis using Maximum Likelihood.
@item mlmRev
Examples from Multilevel Modelling Software Review.
@item mmlcr
Mixed-mode latent class regression (also known as mixed-mode mixture
model regression or mixed-mode mixture regression models) which can
handle both longitudinal and one-time responses.
@item moc
Fits a variety of mixtures models for multivariate observations with
user-difined distributions and curves.
@item modeltools
A collection of tools to deal with statistical models.
@item mscalib
Calibration and filtering of MALDI-TOF Peptide Mass Fingerprint data.
@item msm
Functions for fitting continuous-time Markov multi-state models to
categorical processes observed at arbitrary times, optionally with
misclassified responses, and covariates on transition or
misclassification rates.
@item muhaz
Hazard function estimation in survival analysis.
@item multcomp
Multiple comparison procedures for the one-way layout.
@c <COMMENT>
@c Removed from CRAN on 2004-10-20 due to possible legal problems.
@c @item multidim
@c Multidimensional descriptive statistics: factorial methods and
@c classification.
@c </COMMENT>
@item multinomRob
Overdispersed multinomial regression using robust (LQD and tanh)
estimation.
@c <COMMENT>
@c Moved to the Archive on 2004-09-30.
@c @item multiv
@c Functions for hierarchical clustering, partitioning, bond energy
@c algorithm, Sammon mapping, PCA and correspondence analysis.
@c </COMMENT>
@item multtest
Resampling-based multiple hypothesis testing.
@item mvbutils
Utilities by Mark V. Bravington for project organization, editing and
backup, sourcing, documentation (formal and informal), package
preparation, macro functions, and more.
@item mvnmle
ML estimation for multivariate normal data with missing values.
@item mvnormtest
Generalization of the Shapiro-Wilk test for multivariate variables.
@item mvoutlier
Multivariate outlier detection based on robust estimates of location and
covariance structure.
@item mvpart
Multivariate partitioning.
@item mvtnorm
Multivariate normal and @math{t} distributions.
@item nFDR
Nonparametric Estimate of FDR Based on Bernstein polynomials.
@item ncdf
Interface to Unidata netCDF data files.
@item ncomplete
Functions to perform the regression depth method (RDM) to binary
regression to approximate the minimum number of observations that can be
removed such that the reduced data set has complete separation.
@item ncvar
High-level R interface to netCDF datasets.
@item negenes
Estimating the number of essential genes in a genome on the basis of
data from a random transposon mutagenesis experiment, through the use of
a Gibbs sampler.
@c <COMMENT>
@c Moved to the Archive as suggested by the maintainer: we now have
@c ncdf, and other CRAN packages no longer use netCDF.
@c @item netCDF
@c Read data from netCDF files.
@c </COMMENT>
@item neural
RBF and MLP neural networks with graphical user interface.
@item nice
Get or set UNIX priority (niceness) of running R process.
@item nlme
Fit and compare Gaussian linear and nonlinear mixed-effects models.
@emph{Recommended}.
@item nlmeODE
Combine the @strong{nlme} and @strong{odesolve} packages for
mixed-effects modelling using differential equations.
@item nlrq
Nonlinear quantile regression routines.  @emph{Defunct}.
@item nnet
Software for single hidden layer perceptrons (``feed-forward neural
networks''), and for multinomial log-linear models.  Contained in the
@file{VR} bundle.  @emph{Recommended}.
@item nor1mix
One-dimensional normal mixture models classes, for, e.g., density
estimation or clustering algorithms research and teaching; providing the
widely used Marron-Wand densities.
@item norm
Analysis of multivariate normal datasets with missing values.
@item normalp
A collection of utilities for normal of order @math{p} distributions
(General Error Distributions).
@item nortest
Five omnibus tests for the composite hypothesis of normality.
@item noverlap
Functions to perform the regression depth method (RDM) to binary
regression to approximate the amount of overlap, i.e., the minimal
number of observations that need to be removed such that the reduced
data set has no longer overlap.
@item npmc
Nonparametric Multiple Comparisons:  provides simultaneous rank test
procedures for the one-way layout without presuming a certain
distribution.
@item nprq
Nonparametric quantile regression.  @emph{Defunct}.
@item odesolve
An interface for the Ordinary Differential Equation (ODE) solver lsoda.
ODEs are expressed as R functions.
@item orientlib
Representations, conversions and display of orientation SO(3) data.
@item ouch
Ornstein-Uhlenbeck models for phylogenetic comparative hypotheses.
@item outliers
A collection of some tests commonly used for identifying outliers.
@item oz
Functions for plotting Australia's coastline and state boundaries.
@item pamr
Pam: Prediction Analysis for Microarrays.
@item pan
Multiple imputation for multivariate panel or clustered data.
@item panel
Functions and datasets for fitting models to Panel data.
@item pastecs
Package for Analysis of Space-Time Ecological Series.
@item pcurve
Fits a principal curve to a numeric multivariate dataset in arbitrary
dimensions.  Produces diagnostic plots.  Also calculates Bray-Curtis and
other distance matrices and performs multi-dimensional scaling and
principal component analyses.
@item pear
Periodic Autoregression Analysis.
@item permax
Functions intended to facilitate certain basic analyses of DNA array
data, especially with regard to comparing expression levels between two
types of tissue.
@item perturb
Perturbation analysis for evaluating collinearity.
@item pgam
Poisson-Gamma Additive Models.
@item pheno
Some easy-to-use functions for time series analyses of (plant-)
phenological data sets.
@item phpSerialize
Serialize R to PHP associative array.
@item phyloarray
Software to process data from phylogenetic or identification
microarrays.
@item pinktoe
Converts S trees to @HTML{}/Perl files for interactive tree traversal.
@item pixmap
Functions for import, export, plotting and other manipulations of
bitmapped images.
@item plotrix
Various useful functions for enhancing plots.
@item plugdensity
Kernel density estimation with global bandwidth selection via
``plug-in''.
@item pls
Partial Least Squares Regression (PLSR) and Principal Component
Regression (PCR).
@item pls.pcr
Multivariate regression by PLS and PCR.
@item plsgenomics
PLS analyses for genomics.
@c <COMMENT>
@c Outdated on CRAN along with 1.5.0 release as we do not have an active
@c maintainer to fix outstanding quality problems.
@c @item polymars
@c Polychotomous regression based on Multivariate Adaptive Regression
@c Splines.
@c </COMMENT>
@item polspline
Routines for the polynomial spline fitting routines hazard regression,
hazard estimation with flexible tails, logspline, lspec, polyclass, and
polymars, by C. Kooperberg and co-authors.
@item polycor
Polychoric and polyserial correlations.
@item polynom
A collection of functions to implement a class for univariate polynomial
manipulations.
@item ppc
Sample classification of protein mass spectra by peak probabilty
contrasts.
@item pps
Functions to select samples using PPS (probability proportional to size)
sampling, for stratified simple random sampling, and to compute joint
inclusion probabilities for Sampford's method of PPS sampling.
@item prabclus
Distance based parametric bootstrap tests for clustering, mainly thought
for presence-absence data (clustering of species distribution maps).
Jaccard and Kulczynski distance measures, clustering of MDS scores, and
nearest neighbor based noise detection.
@item princurve
Fits a principal curve to a matrix of points in arbitrary dimension.
@item proto
An object oriented system using prototype or object-based (rather than
class-based) object oriented ideas.
@item pscl
R in the Political Science Computational Laboratory, Stanford
University.
@item pspline
Smoothing splines with penalties on order @math{m} derivatives.
@item psy
Various procedures used in psychometry: Kappa, ICC, Cronbach alpha,
screeplot, PCA and related methods.
@item pwt
The Penn World Table providing purchasing power parity and national
income accounts converted to international prices for 168 countries for
some or all of the years 1950--2000.
@item pvclust
Hierarchical clustering with @math{p}-value.
@item qcc
Quality Control Charts.  Shewhart quality control charts for continuous,
attribute and count data.  Cusum and EWMA charts.  Operating
characteristic curves.  Process capability analysis.  Pareto chart and
cause-and-effect chart.
@item qtl
Analysis of experimental crosses to identify QTLs.
@item qtlDesign
Tools for the design of QTL experiments.
@item quadprog
For solving quadratic programming problems.
@item quantreg
Quantile regression and related methods.
@item qvalue
Q-value estimation for false discovery rate control.
@item qvcalc
Functions to compute quasi-variances and associated measures of
approximation error.
@item race
Implementation of some racing methods for the empirical selection of the
best.
@item randomForest
Breiman's random forest classifier.
@c @item ratetables
@c US national and state mortality data (requires @strong{survival4} and
@c @strong{date}), for use with @strong{survival4}.
@item rbugs
Functions to prepare files needed for running BUGS in batch mode, and
running BUGS from R.  Support for Linux systems with Wine is emphasized.
@item rcdd
C Double Description for R, an interface to the CDD computational
geometry library.
@item ref
Functions for creating references, reading from and writing ro
references and a memory efficient refdata type that transparently
encapsulates matrices and data frames.
@item regress
Fitting Gaussian linear models where the covariance structure is a
linear combination of known matrices by maximising the residual log
likelihood.  Can be used for multivariate models and random effects
models.
@item relax
Functions for report writing, presentation, and programming.
@item reldist
Functions for the comparison of distributions, including nonparametric
estimation of the relative distribution PDF and CDF and numerical
summaries as described in ``Relative Distribution Methods in the Social
Sciences'' by Mark S. Handcock and Martina Morris, 1999, Springer.
@item relimp
Functions to facilitate inference on the relative importance of
predictors in a linear or generalized linear model.
@item relsurv
Various functions for regression in relative survival.
@item resper
Sampling from restricted permutations.
@item rgdal
Provides bindings to Frank Warmerdam's Geospatial Data Abstraction 
Library (GDAL).
@item rgenoud
R version of GENetic Optimization Using Derivatives.
@item rgl
3D visualization device system (OpenGL).
@item rimage
Functions for image processing, including Sobel filter, rank filters,
fft, histogram equalization, and reading @acronym{JPEG} files.
@item rlecuyer
R interface to RNG with multiple streams.
@item rmeta
Functions for simple fixed and random effects meta-analysis for
two-sample comparison of binary outcomes.
@item rmetasim
An interface between R and the metasim simulation engine.  Facilitates
the use of the metasim engine to build and run individual based
population genetics simulations.
@item rpart
Recursive PARTitioning and regression trees.  @emph{Recommended}.
@item rpart.permutation
Permutation tests of rpart models.
@item rpvm
R interface to PVM (Parallel Virtual Machine).  Provides interface to
PVM APIs, and examples and documentation for its use.
@item rqmcmb2
Markov chain marginal bootstrap for quantile regression.
@item rrcov
Functions for robust location and scatter estimation and robust
regression with high breakdown point.
@item rsprng
Provides interface to SPRNG (Scalable Parallel Random Number Generators)
APIs, and examples and documentation for its use.
@item rstream
Unified object oriented interface for multiple independent streams of
random numbers from different sources.
@item rwt
Rice Wavelet Toolbox wrapper, providing a set of functions for
performing digital signal processing.
@item sac
Semiparametric empirical likelihood ratio based test of changepoint with
one-change or epidemic alternatives with data-based model diagnostic.
@item sampling
A set of tools to select and to calibrate samples.
@item sampfling
Implements a modified version of the Sampford sampling algorithm.  Given
a quantity assigned to each unit in the population, samples are drawn
with probability proportional to te product of the quantities of the
units included in the sample.
@item samr
Significance Analysis of Microarrays.
@item sandwich
Model-robust standard error estimators for time series and longitudinal
data.
@item sca
Simple Component Analysis.
@item scatterplot3d
Plots a three dimensional (3D) point cloud perspectively.
@item seacarb
Calculates parameters of the seawater carbonate system.
@item seao
Simple Evolutionary Algorithm Optimization.
@item seao.gui
Simple Evolutionary Algorithm Optimization: graphical user interface.
@item segmented
Functions to estimate break-points of segmented relationships in
regression models (GLMs).
@item sem
Functions for fitting general linear Structural Equation Models (with
observed and unobserved variables) by the method of maximum likelihood
using the RAM approach.
@item seqinr
Exploratory data analysis and data visualization for biological sequence
(DNA and protein) data.
@item seqmon
Sequential monitoring of clinical trials.
@c <COMMENT>
@c Moved to the Archive as requested by the maintainer: serialize() et
@c al are available in R 1.8 or better.
@c @item serialize
@c Simple interfce for serializing to connections.
@c </COMMENT>
@item session
Functions for interacting with, saving and restoring R sessions.
@item setRNG
Set (normal) random number generator and seed.
@item sfsmisc
Utilities from Seminar fuer Statistik ETH Zurich.
@item sgeostat
An object-oriented framework for geostatistical modeling.
@item shapefiles
Functions to read and write ESRI shapefiles.
@item shapes
Routines for the statistical analysis of shapes, including procrustes
analysis, displaying shapes and principal components, testing for mean
shape difference, thin-plate spline transformation grids and edge
superimposition methods.
@item simpleboot
Simple bootstrap routines.
@item skewt
Density, distribution function, quantile function and random generation
for the skewed @math{t} distribution of Fernandez and Steel.
@item sm
Software linked to the book ``Applied Smoothing Techniques for Data
Analysis:  The Kernel Approach with @SPLUS{} Illustrations'' by
A. W. Bowman and A. Azzalini, 1997, Oxford University Press.
@item sma
Functions for exploratory (statistical) microarray analysis.
@item smoothSurv
Survival regression with smoothed error distribution.
@item sn
Functions for manipulating skew-normal probability distributions and for
fitting them to data, in the scalar and the multivariate case.
@item sna
A range of tools for social network analysis, including node and
graph-level indices, structural distance and covariance methods,
structural equivalence detection, p* modeling, and network
visualization.
@c <COMMENT>
@c Removed ...
@c @item snns
@c An R interface to the Stuttgart Neural Networks Simulator (SNNS).
@c </COMMENT>
@item snow
Simple Network of Workstations: support for simple parallel computing in
R.
@item snowFT
Fault Tolerant Simple Network of Workstations.
@item som
Self-Organizing Maps (with application in gene clustering).
@item sound
A sound interface for R: Basic functions for dealing with @file{.wav}
files and sound samples.
@item sp
A package that provides classes and methods for spatial data, including
utility functions for plotting data as maps, spatial selection, amd much
more.
@item spatial
Functions for kriging and point pattern analysis from ``Modern Applied
Statistics with S'' by W. Venables and B. Ripley.  Contained in the
@file{VR} bundle.  @emph{Recommended}.
@item spatialCovariance
Computation of spatial covariance matrices for data on rectangles using
one dimensional numerical integration and analytic results.
@item spatstat
Data analysis and modelling of two-dimensional point patterns, including
multitype points and spatial covariates.
@item spc
Statistical Process Control: evaluation of control charts by means of
the zero-state, steady-state ARL (Average Run Length), setting up
control charts for given in-control ARL, and plotting of the related
figures.
@item spdep
A collection of functions to create spatial weights matrix objects from
polygon contiguities, from point patterns by distance and tesselations,
for summarising these objects, and for permitting their use in spatial
data analysis; a collection of tests for spatial autocorrelation,
including global Moran's I and Geary's C, local Moran's I, saddlepoint
approximations for global and local Moran's I; and functions for
estimating spatial simultaneous autoregressive (SAR) models.  (Was
formerly the three packages: @strong{spweights}, @strong{sptests}, and
@strong{spsarlm}.)
@item spe
Stochastic Proximity Embedding.
@item spectrino
Spectra organizer, visualization and data extraction from within R.
@item splancs
Spatial and space-time point pattern analysis functions.
@c <COMMENT>
@c Deprecated in favor of new @strong{spdep} along with 1.5.0 release.
@c @item spsarlm
@c Functions for estimating spatial simultaneous autoregressive (SAR)
@c models, including sparse matrix methods for computing the Jacobian.
@c @item sptests
@c A collection of tests for spatial autocorrelation, including global
@c Moran's I and Geary's C.
@c @item spweights
@c A collection of functions to create spatial weights matrix objects from
@c polygon contiguities, from point patterns by distance and tesselations,
@c for summarising these objects, and for permitting their use in spatial
@c data analysis.
@c </COMMENT>
@item sspir
State SPace models In R.
@c @item stable
@c Density, distribution, quantile and hazard functions of a stable
@c variate; generalized linear models for the parameters of a stable
@c distribution.
@c <COMMENT>
@c Merged into foreign.
@c @item stataread
@c Read and write Stata v5 and v6 @file{.dta} files.
@c </COMMENT>
@item statmod
Miscellaneous biostatistical modelling functions.
@item stepwise
A stepwise approach to identifying recombination breakpoints in a
sequence alignment.
@item strucchange
Various tests on structural change in linear regression models.
@item subselect
A collection of functions which assess the quality of variable subsets
as surrogates for a full data set, and search for subsets which are
optimal under various criteria.
@item supclust
Methodology for supervised grouping of predictor variables.
@item superpc
Supervised principal components.
@item survBayes
Fits a proportional hazards model to time to event data by a Bayesian
approach.
@item survey
Summary statistics, generalized linear models, and general maximum
likelihood estimation for stratified, cluster-sampled, unequally
weighted survey samples.
@c @item survival4
@c Functions for survival analysis, version 4 (requires
@c @strong{splines}).
@item survival
Functions for survival analysis, including penalised likelihood.
@emph{Recommended}.
@item survrec
Survival analysis for recurrent event data.
@item svmpath
Computes the entire regularization path for the two-class svm classifier
with essentialy the same cost as a single SVM fit.
@item systemfit
Contains functions for fitting simultaneous systems of equations using
Ordinary Least Sqaures (OLS), Two-Stage Least Squares (2SLS), and
Three-Stage Least Squares (3SLS).
@item tapiR
Tools for accessing (UK) parliamentary information in R.
@item taskPR
Task-Parallel R package.
@item tdist
Computes the distribution of a linear combination of independent
Student's @math{t} variables.
@item tensor
Tensor product of arrays.
@item time
Time tracking for developers.
@item tkrplot
Simple mechanism for placing R graphics in a Tk widget.
@item tree
Classification and regression trees.
@item treeglia
Stem analysis functions for volume increment and carbon uptake
assessment from tree-rings.
@item tripack
A constrained two-dimensional Delaunay triangulation package.
@item tseries
Package for time series analysis with emphasis on non-linear modelling.
@item tuneR
Collection of tools to analyze music, handle wave files, transcription,
etc.
@item tweedie
Maximum likelihood computations for Tweedie exponential family models.
@item twostage
Functions for optimal design of two-stage-studies using the Mean Score
method.
@item udunits
Interface to Unidata's routines to convert units.
@item ump
Uniformly Most Powerful tests.
@item urca
Unit root and cointegration tests for time series data.
@item urn
Functions for sampling without replacement (simulated urns).
@item uroot
Unit root tests and graphics for seasonal time series.
@item vabayelMix
Variational Bayesian mixture model.
@item varSelRF
Variable selection using random forests.
@item vardiag
Interactive variogram diagnostics.
@item vcd
Functions and data sets based on the book ``Visualizing Categorical
Data'' by Michael Friendly.
@item vegan
Various help functions for vegetation scientists and community
ecologists.
@item verification
Utilities for verification of discrete and probabilistic forecasts.
@item verify
Construction of test suites using verify objects.
@item vioplot
Violin plots, which are a combination of a box plot and a kernel density
plot.
@item waveslim
Basic wavelet routines for time series analysis.
@item wavethresh
Software to perform 1-d and 2-d wavelet statistics and transforms.
@item wle
Robust statistical inference via a weighted likelihood approach.
@item xgobi
Interface to the XGobi and XGvis programs for graphical data analysis.
@item xtable
Export data to @LaTeX{} and @HTML{} tables.
@item zicounts
Fit classical, zero-inflated and interval censored count data regression
models.
@item zoo
A class with methods for totally ordered indexed observations such as
irregular time series.
@end table

@noindent
See @CRAN{} @file{src/contrib/PACKAGES} for more information.

There is also a @CRAN{} @file{src/contrib/Devel} directory which
contains packages still ``under development'' or depending on features
only present in the current development versions of R.  Volunteers are
invited to give these a try, of course.  This area of @CRAN{} currently
contains

@table @strong
@c <COMMENT>
@c Moved to Attic on 2004-12-14 as requested by maintainer.
@c @item Dopt
@c Finding D-optimal experimental designs.
@c </COMMENT>

@c <COMMENT>
@c Moved to Devel for 1.8 as it fails R CMD check.
@item GLMMGibbs
Generalised Linear Mixed Models by Gibbs sampling.
@c </COMMENT>

@c <COMMENT>
@c Moved to Devel for 1.6 as it fails R CMD check.
@item RPgSQL
Provides methods for accessing data stored in PostgreSQL tables.
@c </COMMENT>

@c <COMMENT>
@c Moved to Attic for 1.5.1 as no longer needed.
@c (See also the info on rdbi.sourceforge.net.)
@c @item Rdbi
@c Generic framework for database access in R.
@c @item Rdbi.PgSQL
@c Provides methods for accessing data stored in PostgreSQL tables.
@c </COMMENT>

@c <COMMENT>
@c Moved to Attic for 1.5 (as this contains the summary functions).
@c @item colSums
@c Matrix summary functions in C.
@c </COMMENT>

@c <COMMENT>
@c Moved to Attic for 1.5.1 as no longer needed.
@c @item cxx
@c A small C++ test package.
@c </COMMENT>

@item dseplus
Extensions to @strong{dse}, the Dynamic Systems Estimation multivariate
time series package.  Contains PADI, juice and monitoring extensions.

@item ensemble
Ensembles of tree classifiers.

@item rcom
R COM Client Interface and internal COM Server.

@c <COMMENT>
@c Moved to Devel for 1.5 as it fails R CMD check.
@c Moved to Attic for 1.5.1 as apparently no longer actively
@c maintained.
@c @item event.chart
@c Package for creating event charts.
@c </COMMENT>

@c <COMMENT>
@c Moved to Attic for 1.5.1 as no longer maintained.
@c @item hpower
@c A suite of functions to compute power and sample size for tests of the
@c general linear hypothesis.
@c </COMMENT>

@c <COMMENT>
@c Moved to Attic on 2003-09-12 as requested by the package maintainer,
@c "kjetil brinchmann halvorsen" <kjetil@@entelnet.bo>.
@c @item pls
@c Univariate Partial Least Squares Regression.
@c </COMMENT>

@c <COMMENT>
@c Moved to Attic for 1.5.1 as no longer needed.
@c @item regexp
@c Simple regular expression interface.
@c </COMMENT>

@item runStat
Running median and mean.

@item write.snns
Function for writing a @acronym{SNNS} pattern file from a data frame or
matrix.
@end table

@node Add-on packages from Omegahat, Add-on packages from Bioconductor, Add-on packages from CRAN, Which add-on packages exist for R?
@subsection Add-on packages from Omegahat

The @url{http://www.omegahat.org/, Omegahat Project for Statistical
Computing} provides a variety of open-source software for statistical
applications, with special emphasis on web-based software, Java, the
Java virtual machine, and distributed computing.  A @acronym{CRAN} style
R package repository is available via @url{http://www.omegahat.org/R/}.

Currently, there are the following packages.

@table @strong
@item CORBA
Dynamic CORBA client/server facilities for R.  Connects to other
CORBA-aware applications developed in arbitrary languages, on different
machines and allows R functionality to be exported in the same way to
other applications.
@item Combinations
Compute the combinations of choosing @math{r} items from @math{n}
elements.
@item IDocs
Infrastructure for interactive documents.
@item OOP
OOP style classes and methods for R and @SPLUS{}.  Object references and
class-based method definition are supported in the style of languages
such as Java and C++.
@item RCurl
Allows one to compose HTTP requests to fetch URIs, post forms, etc., and
process the results returned by the Web server.
@item RDCOMClient
Provides dynamic client-side access to (D)COM applications from within
R.
@item RDCOMEvents
Provides facilities to use R functions and objects as handlers for DCOM
events.
@item RDCOMServer
Facilities for exporting S objects and functions as COM objects.
@item REmbeddedPostgres
Allows R functions and objects to be used to implement SQL functions ---
per-record, aggregate and trigger functions.
@item REventLoop
An abstract event loop mechanism that is toolkit independent and can be
used to to replace the R event loop.
@item RGdkPixbuf
S language functions to access the facilities in the GdkPixbuf library
for manipulating images.
@item RGnumeric
A plugin for the Gnumeric spreadsheet that allows R functions to be
called from cells within the sheet, automatic recalculation, etc.
@item RGtk
Facilities in the S language for programming graphical interfaces using
Gtk, the Gnome GUI toolkit.
@item RGtkBindingGenerator
A meta-package which generates C and R code to provide bindings to a
Gtk-based library.
@item RGtkExtra
A collection of S functions that provide an interface to the widgets in
the gtk+extra library such as the GtkSheet data-grid display, icon list,
file list and directory tree.
@item RGtkGlade
S language bindings providing an interface to Glade, the interactive
Gnome GUI creator.
@item RGtkHTML
A collection of S functions that provide an interface to creating and
controlling an @HTML{} widget which can be used to display @HTML{}
documents from files or content generated dynamically in S.
@item RGtkViewers
A collection of tools for viewing different S objects, databases, class
and widget hierarchies, S source file contents, etc.
@item RJavaDevice
A graphics device for R that uses Java components and graphics.
@acronym{API}s.
@item RMatlab
A bi-directional interface between R and Matlab.
@item RObjectTables
The C and S code allows one to define R objects to be used as elements
of the search path with their own semantics and facilities for reading
and writing variables.  The objects implement a simple interface via R
functions (either methods or closures) and can access external data,
e.g., in other applications, languages, formats, @dots{}
@item RSMethods
An implementation of S version 4 methods and classes for R, consistent
with the basic material in ``Programming with Data'' by John
M. Chambers, 1998, Springer NY.
@item RSPerl
An interface from R to an embedded, persistent Perl interpreter,
allowing one to call arbitrary Perl subroutines, classes and methods.
@item RSPython
Allows Python programs to invoke S functions, methods, etc., and S code
to call Python functionality.
@item RXLisp
An interface to call XLisp-Stat functions from within R.
@item Rstem
Interface to Snowball implementation of Porter's word stemming
algorithm.
@item SASXML
Example for reading @XML{} files in SAS 8.2 manner.
@item SJava
An interface from R to Java to create and call Java objects and
methods.
@item SLanguage
Functions and C support utilities to support S language programming
that can work in both R and @SPLUS{}.
@item SNetscape
Plugin for Netscape and JavaScript.
@item SSOAP
A client interface to SOAP (Simple Object Access Protocol) servers from
within S.
@item SWinRegistry
Provides access from within R to read and write the Windows registry.
@item SXalan
Process @XML{} documents using @XSL{} functions implemented in R and
dynamically substituting output from R.
@item Slcc
Parses C source code, allowing one to analyze and automatically generate
interfaces from S to that code, including the table of S-accessible
native symbols, parameter count and type information, S constructors
from C objects, call graphs, etc.
@item Sxslt
An extension module for libxslt, the @XML{}-@XSL{} document translator,
that allows @XSL{} functions to be implemented via R functions.
@item XML
Tools for reading @XML{} documents and DTDs.
@end table

@node Add-on packages from Bioconductor, Other add-on packages, Add-on packages from Omegahat, Which add-on packages exist for R?
@subsection Add-on packages from Bioconductor

The @url{http://www.bioconductor.org/, Bioconductor Project} produces an
open source software framework that will assist biologists and
statisticians working in bioinformatics, with primary emphasis on
inference using DNA microarrays.  A @acronym{CRAN} style R package
repository is available via @url{http://www.bioconductor.org/}.

The following R packages are contained in the current release of
Bioconductor, with more packages under development.

@table @strong
@item AnnBuilder
Assemble and process genomic annotation data, from databases such as
GenBank, the Gene Ontology Consortium, LocusLink, UniGene, the UCSC
Human Genome Project.
@item Biobase
Object-oriented representation and manipulation of genomic data (S4
class structure).
@item Biostrings
Class definitions and generics for biological sequences along with
pattern matching algorithms.
@item ChromoViz
Draw gene expression profile onto chromosome using @acronym{SVG}.
@item DEDS
Differential Expression via Distance Summary for microarray data.
@item DNAcopy
Segments DNA copy number data using circular binary segmentation to
detect regions with abnormal copy number.
@item DynDoc
Functionality to create and interact with dynamic documents, vignettes,
and other navigable documents.
@item EBarrays
Empirical Bayes tools for the analysis of replicated microarray data
across multiple conditions.
@item GLAD
Gain and Loss Analysis of DNA.
@item GOstats
Tools for manipulating GO and microarrays.
@item GeneSpring
Functions and class definitions to be able to read and write GeneSpring
specific data objects and convert them to Bioconductor objects.
@item GeneTS
@c CRAN
A package for analysing multiple gene expression time series data.
Currently, implements methods for cell cycle analysis and for inferring
large sparse graphical Gaussian models.
@item GeneTraffic
GeneTraffic R integration functions.
@item GraphAT
Graph theoretic Association Tests.
@item HEM
Heterogeneous Error Model for analysis of microarray data.
@item Icens
Functions for computing the NPMLE for censored and truncated data.
@item KEGGSOAP
Client-side SOAP access KEGG.
@item LPE
Significance analysis of microarray data with small number of replicates
using the Local Pooled Error (LPE) method.
@item MLInterfaces
Uniform interfaces to machine learning code for the exprSet class from
Bioconductor.
@item MeasurementError.cor
Two-stage measurement error model for correlation estimation with
smaller bias than the usual sample correlation.
@item MergeMaid
Cross-study comparison of gene expression array data.
@item OLIN
Optimized Local Intensity-dependent Normalisation of two-color
microarrays.
@item OLINgui
Graphical user interface for @strong{OLIN}.
@item PROcess
Ciphergen SELDI-TOF processing.
@item RBGL
An interface between the graph package and the Boost graph libraries,
allowing for fast manipulation of graph objects in R.
@item RMAGEML
Functionality to handle MAGEML documents.
@item ROC
Receiver Operating Characteristic (ROC) approach for identifying genes
that are differentially expressed in two types of samples.
@item RSNPper
Interface to chip.org::SNPper for SNP-related data.
@item RdbiPgSQL
Methods for accessing data stored in PostgreSQL tables.
@item Rdbi
Generic framework for database access in R.
@item Resourcerer
Read annotation data from TIGR Resourcerer or convert the annotation
data into Bioconductor data package.
@item Rgraphviz
An interface with Graphviz for plotting graph objects in R.
@item Ruuid
Creates Universally Unique ID values (UUIDs) in R.
@item SAGElyzer
Locates genes based on SAGE tags.
@item SNAData
Data from the book ``Social Network Analysis'' by Wasserman & Faust,
1999.
@item aCGH
Classes and functions for Array Comparative Genomic Hybridization data.
@item affy
Methods for Affymetrix Oligonucleotide Arrays.
@item affyPLM
For fitting Probe Level Models.
@item affycomp
Graphics toolbox for assessment of Affymetrix expression measures.
@item affydata
Affymetrix data for demonstration purposes.
@item affylmGUI
Graphical User Interface for affy analysis using package @strong{limma}.
@item affypdnn
Probe Dependent Nearest Neighbors (PDNN) for the affy package.
@item altcdfenvs
Utilities to handle cdfenvs.
@item annaffy
Functions for handling data from Bioconductor Affymetrix annotation data
packages.
@item annotate
Associate experimental data in real time to biological metadata from web
databases such as GenBank, LocusLink and PubMed.  Process and store
query results.  Generate @HTML{} reports of analyses.
@item arrayMagic
Utilities for quality control and processing for two-color cDNA
microarray data.
@item arrayQuality
Performing print-run and array level quality assessment.
@item bim
@c CRAN
Bayesian interval mapping diagnostics:  functions to interpret QTLCart
and Bmapqtl samples.
@item convert
Convert Microarray Data Objects.
@item ctc
Tools to export and import Tree and Cluster to other programs.
@item daMA
Functions for the efficient design of factorial two-color microarray
experiments and for the statistical analysis of factorial microarray
data.
@item ecolitk
Metadata and tools to work with E. coli.
@item edd
Expression density diagnostics: graphical methods and pattern
recognition algorithms for distribution shape classification.
@item exprExternal
Implementation of exprSet using externalVectors.
@item externalVector
Basic class definitions and generics for external pointer based vector
objects for R.
@item factDesign
A set of tools for analyzing data from factorial designed microarray
experiments.  The functions can be used to evaluate appropriate tests of
contrast and perform single outlier detection.
@item gcrma
Background adjustment using sequence information.
@item genArise
A tool for dual color microarray data.
@item genefilter
Tools for sequentially filtering genes using a wide variety of filtering
functions.  Example of filters include: number of missing value,
coefficient of variation of expression measures, ANOVA @math{p}-value,
Cox model @math{p}-values.  Sequential application of filtering
functions to genes.
@item geneplotter
Graphical tools for genomic data, for example for plotting expression
data along a chromosome or producing color images of expression data
matrices.
@item globaltest
Testing globally whether a group of genes is significantly related to
some clinical variable of interest.
@item goCluster
Analysis of clustering results in conjunction with annotation data.
@item goTools
Functions for description/comparison of oligo ID list using the Gene
Ontology database.
@item gpls
Classification using generalized partial least squares for two-group and
multi-group classification.
@item graph
Classes and tools for creating and manipulating graphs within R.
@item gtkWidgets
Widgets built using @strong{RGtk}.
@item hexbin
Binning functions, in particular hexagonal bins for graphing.
@item hopach
Hierarchical Ordered Partitioning and Collapsing Hybrid (HOPACH).
@item iSPlot
Link views that are based on the same data set.
@item impute
@c CRAN
Imputation for microarray data (currently KNN only).
@item limma
Linear models for microarray data.
@item limmaGUI
Graphical User Interface for package @strong{limma}.
@item makecdfenv
Two functions.  One reads a Affymetrix chip description file (CDF) and
creates a hash table environment containing the location/probe set
membership mapping.  The other creates a package that automatically loads
that environment. 
@item marray
Exploratory analysis for two-color spotted microarray data.
@c @item marrayClasses
@c Class definitions for pre-normalized and normalized cDNA microarray
@c data.  Basic methods for accessing/replacing, printing, and subsetting.
@c @item marrayInput
@c Functions for reading microarray data into R from different image
@c analysis output files, and probe and target description files.  Widgets
@c are supplied to facilitate and automate data input and the creation of
@c microarray specific R objects for storing these data.
@c @item marrayNorm
@c Functions for location and scale normalization procedures based on
@c robust local regression.
@c @item marrayPlots
@c Functions for diagnostic plots for pre- and post-normalization cDNA
@c microarray intensity data: boxplots, scatter-plots, color images.
@c @item marrayTools
@c Miscellaneous functions used in the functional genomics core facility in
@c UCB and UCSF.
@item matchprobes
Tools for sequence matching of probes on arrays.
@item msbase
Basic classes and methods for mass spectrometric mass list
manipulation.
@item multtest
Multiple testing procedures for controlling the family-wise error rate
(FWER) and the false discovery rate (FDR).  Tests can be based on
@math{t}- or @math{F}-statistics for one- and two-factor designs, and
permutation procedures are available to estimate adjusted
@math{p}-values.
@item nnNorm
Spatial and intensity based normalization of cDNA microarray data based
on robust neural nets.
@c @item ontoTools
@c Graphs and sparse matrices for working with ontologies.
@item pairseqsim
Pairwise sequence alignment and scoring algorithms for global, local and 
overlap alignment with affine gap penalty.
@item pamr
@c CRAN
Pam: Prediction Analysis for Microarrays.
@item pickgene
Adaptive gene picking for microarray expression data analysis.
@item prada
Tools for analyzing and navigating data from high-throughput phenotyping
experiments based on cellular assays and fluorescent detection.
@item qvalue
Q-value estimation for false discovery rate control.
@item rama
Robust Analysis of MicroArrays: robust estimation of cDNA microarray
intensities with replicates using a Bayesian hierarchical model. 
@item reposTools
Tools for dealing with file repositories and allow users to easily
install, update, and distribute packages, vignettes, and other files.
@c @item rhdf5
@c Storage and retrieval of large datasets using the HDF5 library and file
@c format.
@item siggenes
Identifying differentially expressed genes and estimating the False
Discovery Rate (FDR) with both the Significance Analysis of Microarrays
(SAM) and the Empirical Bayes Analyses of Microarrays (EBAM).
@item simpleaffy
Very simple high level analysis of Affymetrix data.
@item splicegear
A set of tools to work with alternative splicing.
@item stam
STructured Analysis of Microarray data.
@item stepNorm
Stepwise normalization functions for cDNA microarrays.
@item tkWidgets
Widgets in Tcl/Tk that provide functionality for Bioconductor packages.
@item twilight
Estimation of local false discovery rate.
@item vsn
Calibration and variance stabilizing transformations for both Affymetrix
and cDNA array data.
@item webbioc
Integrated web interface for doing microarray analysis using several of
the Bioconductor packages.
@item widgetInvoke
Evaluation widgets for functions.
@item widgetTools
Tools for creating Tcl/Tk widgets, i.e., small-scale graphical user
interfaces.
@end table

@c These packages will eventually also be made available via @CRAN{}
@c as well.

@node Other add-on packages,  , Add-on packages from Bioconductor, Which add-on packages exist for R?
@subsection Other add-on packages

@email{jlindsey@@luc.ac.be, Jim Lindsey} has written a collection of R
packages for nonlinear regression and repeated measurements, consisting
of @strong{event} (event history procedures and models), @strong{gnlm}
(generalized nonlinear regression models), @strong{growth} (multivariate
normal and elliptically-contoured repeated measurements models),
@strong{repeated} (non-normal repeated measurements models),
@strong{rmutil} (utilities for nonlinear regression and repeated
measurements), and @strong{stable} (probability functions and
generalized regression models for stable distributions).  All analyses
in the new edition of his book ``Models for Repeated Measurements''
(1999, Oxford University Press) were carried out using these packages.
Jim has also started @strong{dna}, a package with procedures for the
analysis of DNA sequences.  Jim's packages can be obtained from
@url{http://www.luc.ac.be/~jlindsey/rcode.html}.

More code has been posted to the R-help mailing list, and can be
obtained from the mailing list archive.

@node How can add-on packages be installed?, How can add-on packages be used?, Which add-on packages exist for R?, R Add-On Packages
@section How can add-on packages be installed?

(Unix only.)  The add-on packages on @CRAN{} come as gzipped tar
files named @code{@var{pkg}_@var{version}.tar.gz}, which may in fact be
``bundles'' containing more than one package.  Provided that
@command{tar} and @command{gzip} are available on your system, type

@example
$ R CMD INSTALL /path/to/@var{pkg}_@var{version}.tar.gz
@end example

@noindent
at the shell prompt to install to the library tree rooted at the first
directory given in @env{R_LIBS} (see below) if this is set and non-null,
and to the default library (the @file{library} subdirectory of
@file{@env{R_HOME}}) otherwise.  (Versions of R prior to 1.3.0 installed
to the default library by default.)

To install to another tree (e.g., your private one), use

@example
$ R CMD INSTALL -l @var{lib} /path/to/@var{pkg}_@var{version}.tar.gz
@end example

@noindent
where @var{lib} gives the path to the library tree to install to.

Even more conveniently, you can install and automatically update
packages from within R if you have access to @CRAN{}.  See the
help page for @code{CRAN.packages()} for more information.

You can use several library trees of add-on packages.  The easiest way
to tell R to use these is via the environment variable @env{R_LIBS}
which should be a colon-separated list of directories at which R library
trees are rooted.  You do not have to specify the default tree in
@env{R_LIBS}.  E.g., to use a private tree in @file{$HOME/lib/R} and a
public site-wide tree in @file{/usr/local/lib/R-contrib}, put

@example
R_LIBS="$HOME/lib/R:/usr/local/lib/R-contrib"; export R_LIBS
@end example

@noindent
into your (Bourne) shell profile or even preferably, add the line

@example
R_LIBS="~/lib/R:/usr/local/lib/R-contrib"
@end example

@noindent
your @file{~/.Renviron} file.  (Note that no @code{export} statement is
needed or allowed in this file; see the on-line help for @code{Startup}
for more information.)

@node How can add-on packages be used?, How can add-on packages be removed?, How can add-on packages be installed?, R Add-On Packages
@section How can add-on packages be used?

To find out which additional packages are available on your system, type

@example
library()
@end example

@noindent
at the R prompt.  

This produces something like

@quotation
@cartouche
@smallexample
Packages in `/home/me/lib/R':

mystuff       My own R functions, nicely packaged but not documented

Packages in `/usr/local/lib/R/library':

KernSmooth    Functions for kernel smoothing for Wand & Jones (1995)
MASS          Main Package of Venables and Ripley's MASS
base          The R Base package
boot          Bootstrap R (S-Plus) Functions (Canty)
class         Functions for Classification
cluster       Functions for clustering (by Rousseeuw et al.)
datasets      The R datasets Package
foreign       Read data stored by Minitab, S, SAS, SPSS, Stata, ...
grDevices     The R Graphics Devices and Support for Colours and Fonts
graphics      The R Graphics Package
grid          The Grid Graphics Package
lattice       Lattice Graphics
methods       Formal Methods and Classes
mgcv          GAMs with GCV smoothness estimation and GAMMs by REML/PQ
nlme          Linear and nonlinear mixed effects models
nnet          Feed-forward Neural Networks and Multinomial Log-Linear
              Models
rpart         Recursive partitioning
spatial       Functions for Kriging and Point Pattern Analysis
splines       Regression Spline Functions and Classes
stats         The R Stats Package
stats4        Statistical functions using S4 classes
survival      Survival analysis, including penalised likelihood
tcltk         Tcl/Tk Interface
tools         Tools for Package Development
utils         The R Utils Package
@end smallexample
@end cartouche
@end quotation

You can ``load'' the installed package @var{pkg} by

@example
library(@var{pkg})
@end example

You can then find out which functions it provides by typing one of

@example
library(help = @var{pkg})
help(package = @var{pkg})
@end example

You can unload the loaded package @var{pkg} by

@example
detach("package:@var{pkg}")
@end example

@node How can add-on packages be removed?, How can I create an R package?, How can add-on packages be used?, R Add-On Packages
@section How can add-on packages be removed?

Use

@example
$ R CMD REMOVE @var{pkg_1} @dots{} @var{pkg_n}
@end example

@noindent
to remove the packages @var{pkg_1}, @dots{}, @var{pkg_n} from the
library tree rooted at the first directory given in @env{R_LIBS} if this
is set and non-null, and from the default library otherwise.  (Versions
of R prior to 1.3.0 removed from the default library by default.)

To remove from library @var{lib}, do

@example
$ R CMD REMOVE -l @var{lib} @var{pkg_1} @dots{} @var{pkg_n}
@end example

@node How can I create an R package?, How can I contribute to R?, How can add-on packages be removed?, R Add-On Packages
@section How can I create an R package?

A package consists of a subdirectory containing the files
@file{DESCRIPTION} and @file{INDEX}, and the subdirectories @file{R},
@file{data}, @file{demo}, @file{exec}, @file{inst}, @file{man},
@file{src}, and @file{tests} (some of which can be missing).  Optionally
the package can also contain script files @file{configure} and
@file{cleanup} which are executed before and after installation.

@ifclear UseExternalXrefs
See section ``Creating R packages'' in @cite{Writing R Extensions}, for
details.  This manual is included in the R distribution, @pxref{What
documentation exists for R?}, and gives information on package
structure, the configure and cleanup mechanisms, and on automated
package checking and building.
@end ifclear
@ifset UseExternalXrefs
@xref{Creating R packages, , Creating R packages, R-exts, Writing R
Extensions}, for details.
@end ifset

R version 1.3.0 has added the function @code{package.skeleton()} which
will set up directories, save data and code, and create skeleton help
files for a set of R functions and datasets.

@xref{What is CRAN?}, for information on uploading a package to @CRAN{}.

@node How can I contribute to R?,  , How can I create an R package?, R Add-On Packages
@section How can I contribute to R?

R is in active development and there is always a risk of bugs creeping
in.  Also, the developers do not have access to all possible machines
capable of running R.  So, simply using it and communicating problems is
certainly of great value.

One place where functionality is still missing is the modeling software
as described in ``Statistical Models in S'' (see @ref{What is S?}); some
of the nonlinear modeling code is not there yet.

The @url{http://developer.R-project.org/, R Developer Page} acts as an
intermediate repository for more or less finalized ideas and plans for
the R statistical system.  It contains (pointers to) TODO lists, RFCs,
various other writeups, ideas lists, and CVS miscellanea.

Many (more) of the packages available at the Statlib S Repository might
be worth porting to R.

If you are interested in working on any of these projects, please notify
@email{Kurt.Hornik@@R-project.org, Kurt Hornik}.

@node R and Emacs, R Miscellanea, R Add-On Packages, Top
@chapter R and Emacs

@menu
* Is there Emacs support for R?::  
* Should I run R from within Emacs?::  
* Debugging R from within Emacs::  
@end menu

@node Is there Emacs support for R?, Should I run R from within Emacs?, R and Emacs, R and Emacs
@section Is there Emacs support for R?

There is an Emacs package called @acronym{ESS} (``Emacs Speaks
Statistics'') which provides a standard interface between statistical
programs and statistical processes.  It is intended to provide
assistance for interactive statistical programming and data analysis.
Languages supported include: S dialects (R, S 3/4, and @SPLUS{}
3.x/4.x/5.x/6.x/7.x), LispStat dialects (XLispStat, ViSta), SAS, Stata,
and BUGS.

@acronym{ESS} grew out of the need for bug fixes and extensions to
S-mode 4.8 (which was a @acronym{GNU} Emacs interface to S/@SPLUS{}
version 3 only).  The current set of developers desired support for
XEmacs, R, S4, and MS Windows.  In addition, with new modes being
developed for R, Stata, and SAS, it was felt that a unifying interface
and framework for the user interface would benefit both the user and the
developer, by helping both groups conform to standard Emacs usage.  The
end result is an increase in efficiency for statistical programming and
data analysis, over the usual tools.

R support contains code for editing R source code (syntactic indentation
and highlighting of source code, partial evaluations of code, loading
and error-checking of code, and source code revision maintenance) and
documentation (syntactic indentation and highlighting of source code,
sending examples to running @acronym{ESS} process, and previewing),
interacting with an inferior R process from within Emacs (command-line
editing, searchable command history, command-line completion of R object
and file names, quick access to object and search lists, transcript
recording, and an interface to the help system), and transcript
manipulation (recording and saving transcript files, manipulating and
editing saved transcripts, and re-evaluating commands from transcript
files).

The latest stable version of @acronym{ESS} are available via @CRAN{} or
the @url{http://ESS.R-project.org/, ESS web page}.  The @HTML{} version
of the documentation can be found at @url{http://stat.ethz.ch/ESS/}.

@acronym{ESS} comes with detailed installation instructions.

For help with @acronym{ESS}, send email to
@email{ESS-help@@stat.math.ethz.ch}.

Please send bug reports and suggestions on @acronym{ESS} to
@email{ESS-bugs@@stat.math.ethz.ch}.  The easiest way to do this from is
within Emacs by typing @kbd{M-x ess-submit-bug-report} or using the
[ESS] or [iESS] pulldown menus.

@node Should I run R from within Emacs?, Debugging R from within Emacs, Is there Emacs support for R?, R and Emacs
@section Should I run R from within Emacs?

Yes, @emph{definitely}.  Inferior R mode provides a readline/history
mechanism, object name completion, and syntax-based highlighting of the
interaction buffer using Font Lock mode, as well as a very convenient
interface to the R help system.

Of course, it also integrates nicely with the mechanisms for editing R
source using Emacs.  One can write code in one Emacs buffer and send
whole or parts of it for execution to R; this is helpful for both data
analysis and programming.  One can also seamlessly integrate with a
revision control system, in order to maintain a log of changes in your
programs and data, as well as to allow for the retrieval of past
versions of the code.

In addition, it allows you to keep a record of your session, which can
also be used for error recovery through the use of the transcript mode.

To specify command line arguments for the inferior R process, use
@kbd{C-u M-x R} for starting R.

@c This prompts you for the arguments; in particular, you can increase
@c the memory size this way (@pxref{Why does R run out of memory?}).

@node Debugging R from within Emacs,  , Should I run R from within Emacs?, R and Emacs
@section Debugging R from within Emacs

To debug R ``from within Emacs'', there are several possibilities.  To
use the Emacs GUD (Grand Unified Debugger) library with the recommended
debugger GDB, type @kbd{M-x gdb} and give the path to the R
@emph{binary} as argument.  At the @command{gdb} prompt, set
@env{R_HOME} and other environment variables as needed (using e.g.@:
@kbd{set env R_HOME /path/to/R/}, but see also below), and start the
binary with the desired arguments (e.g., @kbd{run --quiet}).

If you have @acronym{ESS}, you can do @kbd{C-u M-x R @key{RET} - d
@key{SPC} g d b @key{RET}} to start an inferior R process with arguments
@option{-d gdb}.

A third option is to start an inferior R process via @acronym{ESS}
(@kbd{M-x R}) and then start GUD (@kbd{M-x gdb}) giving the R binary
(using its full path name) as the program to debug.  Use the program
@command{ps} to find the process number of the currently running R
process then use the @code{attach} command in gdb to attach it to that
process.  One advantage of this method is that you have separate
@code{*R*} and @code{*gud-gdb*} windows.  Within the @code{*R*} window
you have all the @acronym{ESS} facilities, such as object-name
completion, that we know and love.

When using GUD mode for debugging from within Emacs, you may find it
most convenient to use the directory with your code in it as the current
working directory and then make a symbolic link from that directory to
the R binary.  That way @file{.gdbinit} can stay in the directory with
the code and be used to set up the environment and the search paths for
the source, e.g.@: as follows:

@example
set env R_HOME /opt/R
set env R_PAPERSIZE letter
set env R_PRINTCMD lpr
dir /opt/R/src/appl
dir /opt/R/src/main
dir /opt/R/src/nmath
dir /opt/R/src/unix
@end example

@node R Miscellanea, R Programming, R and Emacs, Top
@chapter R Miscellanea

@menu
* How can I set components of a list to NULL?::  
* How can I save my workspace?::  
* How can I clean up my workspace?::  
* How can I get eval() and D() to work?::  
* Why do my matrices lose dimensions?::  
* How does autoloading work?::  
* How should I set options?::   
* How do file names work in Windows?::  
* Why does plotting give a color allocation error?::  
* How do I convert factors to numeric?::  
* Are Trellis displays implemented in R?::  
* What are the enclosing and parent environments?::  
* How can I substitute into a plot label?::  
* What are valid names?::       
* Are GAMs implemented in R?::  
* Why is the output not printed when I source() a file?::  
* Why does outer() behave strangely with my function?::  
* Why does the output from anova() depend on the order of factors in the model?::  
* How do I produce PNG graphics in batch mode?::  
* How can I get command line editing to work?::  
* How can I turn a string into a variable?::  
* Why do lattice/trellis graphics not work?::  
* How can I sort the rows of a data frame?::  
* Why does the help.start() search engine not work?::  
* Why did my .Rprofile stop working when I updated R?::  
* Where have all the methods gone?::  
* How can I create rotated axis labels?::  
* Why is read.table() so inefficient?::  
* What is the difference between package and library?::  
* I installed a package but the functions are not there::  
* Why doesn't R think these numbers are equal?::  
@end menu

@c @node Why does R run out of memory?, Why does sourcing a correct file fail?, R Miscellanea, R Miscellanea
@c @section Why does R run out of memory?

@c Versions of R prior to 1.2.0 used a @emph{static} memory model.  At
@c startup, R asked the operating system to reserve a fixed amount of
@c memory for it.  The size of this chunk could not be changed
@c subsequently.  Hence, it could happen that not enough memory was
@c allocated, e.g., when trying to read large data sets into R.  In such
@c cases, it was necessary to restart R with more memory available, as
@c controlled by the command line options @option{--nsize} and
@c @option{--vsize}.

@c R version 1.2.0 introduces a new ``generational'' garbage collector,
@c which will increase the memory available to R as needed.  Hence, user
@c intervention is no longer necessary for ensuring that enough memory is
@c available.

@c The new garbage collector does not move objects in memory, meaning that
@c it is possible for the free memory to become fragmented so that large
@c objects cannot be allocated even when there is apparently enough memory
@c for them.

@c @node Why does sourcing a correct file fail?, How can I set components of a list to NULL?, Why does R run out of memory?, R Miscellanea
@c @section Why does sourcing a correct file fail?

@c Versions of R prior to 1.2.1 may have had problems parsing files not
@c ending in a newline.  Earlier R versions had a similar problem when
@c reading in data files.  This should no longer happen.

@node How can I set components of a list to NULL?, How can I save my workspace?, R Miscellanea, R Miscellanea
@section How can I set components of a list to NULL?

You can use

@example
x[i] <- list(NULL)
@end example

@noindent
to set component @code{i} of the list @code{x} to @code{NULL}, similarly
for named components.  Do not set @code{x[i]} or @code{x[[i]]} to
@code{NULL}, because this will remove the corresponding component from
the list.

For dropping the row names of a matrix @code{x}, it may be easier to use
@code{rownames(x) <- NULL}, similarly for column names.

@node How can I save my workspace?, How can I clean up my workspace?, How can I set components of a list to NULL?, R Miscellanea
@section How can I save my workspace?

@code{save.image()} saves the objects in the user's @code{.GlobalEnv} to
the file @file{.RData} in the R startup directory.  (This is also what
happens after @kbd{q("yes")}.)  Using @code{save.image(@var{file})} one
can save the image under a different name.

@node How can I clean up my workspace?, How can I get eval() and D() to work?, How can I save my workspace?, R Miscellanea
@section How can I clean up my workspace?

To remove all objects in the currently active environment (typically
@code{.GlobalEnv}), you can do

@example
rm(list = ls(all = TRUE))
@end example

@noindent
(Without @option{all = TRUE}, only the objects with names not starting
with a @samp{.} are removed.)

@node How can I get eval() and D() to work?, Why do my matrices lose dimensions?, How can I clean up my workspace?, R Miscellanea
@section How can I get eval() and D() to work?

Strange things will happen if you use @code{eval(print(x), envir = e)}
or @code{D(x^2, "x")}.  The first one will either tell you that
"@code{x}" is not found, or print the value of the wrong @code{x}.
The other one will likely return zero if @code{x} exists, and an error
otherwise.

This is because in both cases, the first argument is evaluated in the
calling environment first.  The result (which should be an object of
mode @code{"expression"} or @code{"call"}) is then evaluated or
differentiated.  What you (most likely) really want is obtained by
``quoting'' the first argument upon surrounding it with
@code{expression()}.  For example,

@example
R> D(expression(x^2), "x")
2 * x
@end example

Although this behavior may initially seem to be rather strange, is
perfectly logical.  The ``intuitive'' behavior could easily be
implemented, but problems would arise whenever the expression is
contained in a variable, passed as a parameter, or is the result of a
function call.  Consider for instance the semantics in cases like

@example
D2 <- function(e, n) D(D(e, n), n)
@end example

@noindent
or

@example
g <- function(y) eval(substitute(y), sys.frame(sys.parent(n = 2)))
g(a * b)
@end example

See the help page for @code{deriv()} for more examples.

@node Why do my matrices lose dimensions?, How does autoloading work?, How can I get eval() and D() to work?, R Miscellanea
@section Why do my matrices lose dimensions?

When a matrix with a single row or column is created by a subscripting
operation, e.g., @code{row <- mat[2, ]}, it is by default turned into a
vector.  In a similar way if an array with dimension, say, @w{2 x 3 x 1
x 4} is created by subscripting it will be coerced into a @w{2 x 3 x 4}
array, losing the unnecessary dimension.  After much discussion this has
been determined to be a @emph{feature}.

To prevent this happening, add the option @option{drop = FALSE} to the
subscripting.  For example,

@example
rowmatrix <- mat[2, , drop = FALSE]  # @r{creates a row matrix}
colmatrix <- mat[, 2, drop = FALSE]  # @r{creates a column matrix}
a <- b[1, 1, 1, drop = FALSE]        # @r{creates a 1 x 1 x 1 array}
@end example

The @option{drop = FALSE} option should be used defensively when
programming.  For example, the statement

@example
somerows <- mat[index, ]
@end example

@noindent
will return a vector rather than a matrix if @code{index} happens to
have length 1, causing errors later in the code.  It should probably be
rewritten as

@example
somerows <- mat[index, , drop = FALSE]
@end example

@node How does autoloading work?, How should I set options?, Why do my matrices lose dimensions?, R Miscellanea
@section How does autoloading work?

R has a special environment called @code{.AutoloadEnv}.  Using
@kbd{autoload(@var{name}, @var{pkg})}, where @var{name} and
@var{pkg} are strings giving the names of an object and the package
containing it, stores some information in this environment.  When R
tries to evaluate @var{name}, it loads the corresponding package
@var{pkg} and reevaluates @var{name} in the new package's
environment.

Using this mechanism makes R behave as if the package was loaded, but
does not occupy memory (yet).

See the help page for @code{autoload()} for a very nice example.

@node How should I set options?, How do file names work in Windows?, How does autoloading work?, R Miscellanea
@section How should I set options?

The function @code{options()} allows setting and examining a variety of
global ``options'' which affect the way in which R computes and displays
its results.  The variable @code{.Options} holds the current values of
these options, but should never directly be assigned to unless you want
to drive yourself crazy---simply pretend that it is a ``read-only''
variable.

For example, given

@example
test1 <- function(x = pi, dig = 3) @{
  oo <- options(digits = dig); on.exit(options(oo));
  cat(.Options$digits, x, "\n")
@}
test2 <- function(x = pi, dig = 3) @{
  .Options$digits <- dig
  cat(.Options$digits, x, "\n")
@}
@end example

@noindent
we obtain:

@example
R> test1()
3 3.14 
R> test2()
3 3.141593
@end example

What is really used is the @emph{global} value of @code{.Options}, and
using @kbd{options(OPT = VAL)} correctly updates it.  Local copies of
@code{.Options}, either in @code{.GlobalEnv} or in a function
environment (frame), are just silently disregarded.

@node How do file names work in Windows?, Why does plotting give a color allocation error?, How should I set options?, R Miscellanea
@section How do file names work in Windows?

As R uses C-style string handling, @samp{\} is treated as an escape
character, so that for example one can enter a newline as @samp{\n}.
When you really need a @samp{\}, you have to escape it with another
@samp{\}.

Thus, in filenames use something like @code{"c:\\data\\money.dat"}.  You
can also replace @samp{\} by @samp{/} (@code{"c:/data/money.dat"}).

@node Why does plotting give a color allocation error?, How do I convert factors to numeric?, How do file names work in Windows?, R Miscellanea
@section Why does plotting give a color allocation error?

On an X11 device, plotting sometimes, e.g., when running
@code{demo("image")}, results in ``Error: color allocation error''.
This is an X problem, and only indirectly related to R.  It occurs when
applications started prior to R have used all the available colors.
(How many colors are available depends on the X configuration; sometimes
only 256 colors can be used.)

One application which is notorious for ``eating'' colors is Netscape.
If the problem occurs when Netscape is running, try (re)starting it with
either the @option{-no-install} (to use the default colormap) or the
@option{-install} (to install a private colormap) option.

You could also set the @code{colortype} of @code{X11()} to
@code{"pseudo.cube"} rather than the default @code{"pseudo"}.  See the
help page for @code{X11()} for more information.

@c  @node Is R Y2K-compliant?, How do I convert factors to numeric?, Why does plotting give a color allocation error?, R Miscellanea
@c  @section Is R Y2K-compliant?

@c  We expect R to be Y2K compliant when compiled and run on a Y2K compliant
@c  system.  In particular R does not internally represent or manipulate
@c  dates as two-digit quantities.  However, no guarantee of Y2K compliance
@c  is provided for R.  R is free software and comes with @emph{no warranty
@c  whatsoever}.

@c  R, like any other programming language, can be used to write programs
@c  and manipulate data in ways that are not Y2K compliant.

@node How do I convert factors to numeric?, Are Trellis displays implemented in R?, Why does plotting give a color allocation error?, R Miscellanea
@section How do I convert factors to numeric?

It may happen that when reading numeric data into R (usually, when
reading in a file), they come in as factors.  If @code{f} is such a
factor object, you can use

@example
as.numeric(as.character(f))
@end example

@noindent
to get the numbers back.  More efficient, but harder to remember, is

@example
as.numeric(levels(f))[as.integer(f)]
@end example

In any case, do not call @code{as.numeric()} or their likes directly for
the task at hand (as @code{as.numeric()} or @code{unclass()} give the
internal codes).

@node Are Trellis displays implemented in R?, What are the enclosing and parent environments?, How do I convert factors to numeric?, R Miscellanea
@section Are Trellis displays implemented in R?

The recommended package @strong{lattice} (which is based on another
recommended package, @strong{grid}) provides graphical functionality
that is compatible with most Trellis commands.

You could also look at @code{coplot()} and @code{dotchart()} which might
do at least some of what you want.  Note also that the R version of
@code{pairs()} is fairly general and provides most of the functionality
of @code{splom()}, and that R's default plot method has an argument
@code{asp} allowing to specify (and fix against device resizing) the
aspect ratio of the plot.

(Because the word ``Trellis'' has been claimed as a trademark we do not
use it in R.  The name ``lattice'' has been chosen for the R
equivalent.)

@node What are the enclosing and parent environments?, How can I substitute into a plot label?, Are Trellis displays implemented in R?, R Miscellanea
@section What are the enclosing and parent environments?

Inside a function you may want to access variables in two additional
environments: the one that the function was defined in (``enclosing''),
and the one it was invoked in (``parent'').

If you create a function at the command line or load it in a package its
enclosing environment is the global workspace.  If you define a function
@code{f()} inside another function @code{g()} its enclosing environment
is the environment inside @code{g()}.  The enclosing environment for a
function is fixed when the function is created.  You can find out the
enclosing environment for a function @code{f()} using
@code{environment(f)}.

The ``parent'' environment, on the other hand, is defined when you
invoke a function.  If you invoke @code{lm()} at the command line its
parent environment is the global workspace, if you invoke it inside a
function @code{f()} then its parent environment is the environment
inside @code{f()}.  You can find out the parent environment for an
invocation of a function by using @code{parent.frame()} or
@code{sys.frame(sys.parent())}.

So for most user-visible functions the enclosing environment will be the
global workspace, since that is where most functions are defined.  The
parent environment will be wherever the function happens to be called
from.  If a function @code{f()} is defined inside another function
@code{g()} it will probably be used inside @code{g()} as well, so its
parent environment and enclosing environment will probably be the same.

Parent environments are important because things like model formulas
need to be evaluated in the environment the function was called from,
since that's where all the variables will be available.  This relies on
the parent environment being potentially different with each invocation.

Enclosing environments are important because a function can use
variables in the enclosing environment to share information with other
functions or with other invocations of itself (see the section on
lexical scoping).  This relies on the enclosing environment being the
same each time the function is invoked.  (In C this would be done with
static variables.)

Scoping @emph{is} hard.  Looking at examples helps.  It is particularly
instructive to look at examples that work differently in R and S and try
to see why they differ.  One way to describe the scoping differences
between R and S is to say that in S the enclosing environment is
@emph{always} the global workspace, but in R the enclosing environment
is wherever the function was created.

@node How can I substitute into a plot label?, What are valid names?, What are the enclosing and parent environments?, R Miscellanea
@section How can I substitute into a plot label?

Often, it is desired to use the value of an R object in a plot label,
e.g., a title.  This is easily accomplished using @code{paste()} if the
label is a simple character string, but not always obvious in case the
label is an expression (for refined mathematical annotation).  In such a
case, either use @code{parse()} on your pasted character string or use
@code{substitute()} on an expression.  For example, if @code{ahat} is an
estimator of your parameter @math{a} of interest, use

@example
title(substitute(hat(a) == ahat, list(ahat = ahat)))
@end example

@noindent
(note that it is @samp{==} and not @samp{=}).  Sometimes @code{bquote()}
gives a more compact form, e.g.,

@example
title(bquote(hat(a) = .(ahat)))
@end example

@noindent
where subexpressions enclosed in @samp{.()} are replaced by their
values.

There are more worked examples in the mailing list achives.

@node What are valid names?, Are GAMs implemented in R?, How can I substitute into a plot label?, R Miscellanea
@section What are valid names?

When creating data frames using @code{data.frame()} or
@code{read.table()}, R by default ensures that the variable names are
syntactically valid.  (The argument @option{check.names} to these
functions controls whether variable names are checked and adjusted by
@code{make.names()} if needed.)

To understand what names are ``valid'', one needs to take into account
that the term ``name'' is used in several different (but related) ways
in the language:

@enumerate
@item
A @emph{syntactic name} is a string the parser interprets as this type
of expression.  It consists of letters, numbers, and the dot and (for
version of R at least 1.9.0) underscore characters, and starts with
either a letter or a dot not followed by a number.  Reserved words are
not syntactic names.
@item
An @emph{object name} is a string associated with an object that is
assigned in an expression either by having the object name on the left
of an assignment operation or as an argument to the @code{assign()}
function.  It is usually a syntactic name as well, but can be any
non-empty string if it is quoted (and it is always quoted in the call to
@code{assign()}).

@item
An @emph{argument name} is what appears to the left of the equals sign
when supplying an argument in a function call (for example,
@code{f(trim=.5)}).  Argument names are also usually syntactic names,
but again can be anything if they are quoted.

@item
An @emph{element name} is a string that identifies a piece of an object
(a component of a list, for example.)  When it is used on the right of
the @samp{$} operator, it must be a syntactic name, or quoted.
Otherwise, element names can be any strings.  (When an object is used as
a database, as in a call to @code{eval()} or @code{attach()}, the
element names become object names.)

@item
Finally, a @emph{file name} is a string identifying a file in the
operating system for reading, writing, etc.  It really has nothing much
to do with names in the language, but it is traditional to call these
strings file ``names''.
@end enumerate

@node Are GAMs implemented in R?, Why is the output not printed when I source() a file?, What are valid names?, R Miscellanea
@section Are GAMs implemented in R?

Package @strong{gam} from @CRAN{} implements all the Generalized
Additive Models (GAM) functionality as described in the GAM chapter of
the White Book.  In particular, it implements backfitting with both
local regression and smoothing splines, and is extendable.  There is a
@code{gam()} function for GAMs in package @strong{mgcv}, but it is not
an exact clone of what is described in the White Book (no @code{lo()}
for example).  Package @strong{gss} can fit spline-based GAMs too.  And
if you can accept regression splines you can use @code{glm()}.  For
gaussian GAMs you can use @code{bruto()} from package @strong{mda}.

@node Why is the output not printed when I source() a file?, Why does outer() behave strangely with my function?, Are GAMs implemented in R?, R Miscellanea
@section Why is the output not printed when I source() a file?

Most R commands do not generate any output. The command

@example
1+1
@end example

@noindent
computes the value 2 and returns it; the command

@example
summary(glm(y~x+z, family=binomial))
@end example

@noindent
fits a logistic regression model, computes some summary information and
returns an object of class @code{"summary.glm"} (@pxref{How should I
write summary methods?}).

If you type @samp{1+1} or @samp{summary(glm(y~x+z, family=binomial))} at
the command line the returned value is automatically printed (unless it
is @code{invisible()}), but in other circumstances, such as in a
@code{source()}d file or inside a function it isn't printed unless you
specifically print it.

To print the value use

@example
print(1+1)
@end example

@noindent
or

@example
print(summary(glm(y~x+z, family=binomial)))
@end example

@noindent
instead, or use @code{source(@var{file}, echo=TRUE)}.

@node Why does outer() behave strangely with my function?, Why does the output from anova() depend on the order of factors in the model?, Why is the output not printed when I source() a file?, R Miscellanea
@section Why does outer() behave strangely with my function?

As the help for @code{outer()} indicates, it does not work on arbitrary
functions the way the @code{apply()} family does.  It requires functions
that are vectorized to work elementwise on arrays.  As you can see by
looking at the code, @code{outer(x, y, FUN)} creates two large vectors
containing every possible combination of elements of @code{x} and
@code{y} and then passes this to @code{FUN} all at once.  Your function
probably cannot handle two large vectors as parameters.

If you have a function that cannot handle two vectors but can handle two
scalars, then you can still use @code{outer()} but you will need to wrap
your function up first, to simulate vectorized behavior.  Suppose your
function is

@example
foo <- function(x, y, happy) @{
  stopifnot(length(x) == 1, length(y) == 1) # scalars only!
  (x + y) * happy
@}
@end example

@noindent
If you define the general function

@example
wrapper <- function(x, y, my.fun, ...) @{
  sapply(seq(along = x), FUN = function(i) my.fun(x[i], y[i], ...))
@}
@end example

@noindent
then you can use @code{outer()} by writing, e.g.,

@example
outer(1:4, 1:2, FUN = wrapper, my.fun = foo, happy = 10)
@end example

@node Why does the output from anova() depend on the order of factors in the model?, How do I produce PNG graphics in batch mode?, Why does outer() behave strangely with my function?, R Miscellanea
@section Why does the output from anova() depend on the order of factors in the model?

In a model such as @code{~A+B+A:B}, R will report the difference in sums
of squares between the models @code{~1}, @code{~A}, @code{~A+B} and
@code{~A+B+A:B}.  If the model were @code{~B+A+A:B}, R would report
differences between @code{~1}, @code{~B}, @code{~A+B}, and
@code{~A+B+A:B} . In the first case the sum of squares for @code{A} is
comparing @code{~1} and @code{~A}, in the second case it is comparing
@code{~B} and @code{~B+A}.  In a non-orthogonal design (i.e., most
unbalanced designs) these comparisons are (conceptually and numerically)
different.

Some packages report instead the sums of squares based on comparing the
full model to the models with each factor removed one at a time (the
famous `Type III sums of squares' from SAS, for example).  These do not
depend on the order of factors in the model.  The question of which set
of sums of squares is the Right Thing provokes low-level holy wars on
R-help from time to time.

There is no need to be agitated about the particular sums of squares
that R reports.  You can compute your favorite sums of squares quite
easily.  Any two models can be compared with @code{anova(@var{model1},
@var{model2})}, and @code{drop1(@var{model1})} will show the sums of
squares resulting from dropping single terms.

@node How do I produce PNG graphics in batch mode?, How can I get command line editing to work?, Why does the output from anova() depend on the order of factors in the model?, R Miscellanea
@section How do I produce PNG graphics in batch mode?

Under Unix, the @code{png()} device uses the X11 driver, which is a
problem in batch mode or for remote operation.  If you have Ghostscript
you can use @code{bitmap()}, which produces a PostScript file then
converts it to any bitmap format supported by ghostscript.  On some
installations this produces ugly output, on others it is perfectly
satisfactory.  In theory one could also use Xvfb from
@url{http://www.x.org/Downloads.html, X.Org}, which provides an X server
with no display.

@node How can I get command line editing to work?, How can I turn a string into a variable?, How do I produce PNG graphics in batch mode?, R Miscellanea
@section How can I get command line editing to work?

The Unix command-line interface to R can only provide the inbuilt
command line editor which allows recall, editing and re-submission of
prior commands provided that the @acronym{GNU} readline library is
available at the time R is configured for compilation.  Note that the
`development' version of readline including the appropriate headers is
needed: users of Linux binary distributions will need to install
packages such as @code{libreadline-dev} (Debian) or
@code{readline-devel} (Red Hat).

@node How can I turn a string into a variable?, Why do lattice/trellis graphics not work?, How can I get command line editing to work?, R Miscellanea
@section How can I turn a string into a variable?

If you have

@example
varname <- c("a", "b", "d")
@end example

@noindent
you can do

@example
get(varname[1]) + 2
@end example

@noindent
for

@example
a + 2
@end example

@noindent
or

@example
assign(varname[1], 2 + 2)
@end example

@noindent
for

@example
a <- 2 + 2
@end example

@noindent
or

@example
eval(substitute(lm(y ~ x + variable),
                list(variable = as.name(varname[1]))
@end example

@noindent
for

@example
lm(y ~ x + a)
@end example

At least in the first two cases it is often easier to just use a list,
and then you can easily index it by name

@example
vars <- list(a = 1:10, b = rnorm(100), d = LETTERS)
vars[["a"]]
@end example

@noindent
without any of this messing about.

@node Why do lattice/trellis graphics not work?, How can I sort the rows of a data frame?, How can I turn a string into a variable?, R Miscellanea
@section Why do lattice/trellis graphics not work?

The most likely reason is that you forgot to tell R to display the
graph.  Lattice functions such as @code{xyplot()} create a graph object,
but do not display it (the same is true of Trellis graphics in
@SPLUS{}).  The @code{print()} method for the graph object produces the
actual display.  When you use these functions interactively at the
command line, the result is automatically printed, but in
@code{source()} or inside your own functions you will need an explicit
@code{print()} statement.

@node How can I sort the rows of a data frame?, Why does the help.start() search engine not work?, Why do lattice/trellis graphics not work?, R Miscellanea
@section How can I sort the rows of a data frame?

To sort the rows within a data frame, with respect to the values in one
or more of the columns, simply use @code{order()}.

@node Why does the help.start() search engine not work?, Why did my .Rprofile stop working when I updated R?, How can I sort the rows of a data frame?, R Miscellanea
@section Why does the help.start() search engine not work?

The browser-based search engine in @code{help.start()} utilizes a Java
applet.  In order for this to function properly, a compatible version of
Java must installed on your system and linked to your browser, and both
Java @emph{and} JavaScript need to be enabled in your browser.

There have been a number of compatibility issues with versions of Java
and of browsers.  
@ifclear UseExternalXrefs
For further details please consult section ``Enabling search in HTML
help'' in @cite{R Installation and Administration}.  This manual is
included in the R distribution, @pxref{What documentation exists for
R?}, and its @acronym{HTML} version is linked from the @acronym{HTML}
search page.
@end ifclear
@ifset UseExternalXrefs
@xref{Enabling search in HTML help, , Enabling search in HTML help,
R-admin, R Installation and Administration}, for further details.
@end ifset

@node Why did my .Rprofile stop working when I updated R?, Where have all the methods gone?, Why does the help.start() search engine not work?, R Miscellanea
@section Why did my .Rprofile stop working when I updated R?

Did you read the @file{NEWS} file?  For functions that are not in the
@strong{base} package you need to specify the correct package namespace,
since the code will be run @emph{before} the packages are loaded.  E.g.,

@example
ps.options(horizontal = FALSE)
help.start()
@end example

@noindent
needs to be

@example
grDevices::ps.options(horizontal = FALSE)
utils::help.start()
@end example

@c <FIXME>
@c 2.2.0
@noindent
(@code{graphics::ps.options(horizontal = FALSE)} in R 1.9.x).
@c </FIXME>

@node Where have all the methods gone?, How can I create rotated axis labels?, Why did my .Rprofile stop working when I updated R?, R Miscellanea
@section Where have all the methods gone?

Many functions, particularly S3 methods, are now hidden in namespaces.
This has the advantage that they cannot be called inadvertantly with
arguments of the wrong class, but it makes them harder to view.

To see the code for an S3 method (e.g., @code{[.terms}) use

@example
getS3method("[", "terms")
@end example

@noindent
To see the code for an unexported function @code{foo()} in the namespace
of package @code{"bar"} use @code{bar:::foo}.  Don't use these
constructions to call unexported functions in your own code---they are
probably unexported for a reason and may change without warning.

@node How can I create rotated axis labels?, Why is read.table() so inefficient?, Where have all the methods gone?, R Miscellanea
@section How can I create rotated axis labels?

To rotate axis labels (using base graphics), you need to use
@code{text()}, rather than @code{mtext()}, as the latter does not
support @code{par("srt")}.

@example
## @r{Increase bottom margin to make room for rotated labels}
par(mar = c(7, 4, 4, 2) + 0.1)
## @r{Create plot with no x axis and no x axis label}
plot(1 : 8, xaxt = "n",  xlab = "")
## @r{Set up x axis with tick marks alone}
axis(1, labels = FALSE)
## @r{Create some text labels}
labels <- paste("Label", 1:8, sep = " ")
## @r{Plot x axis labels at default tick marks}
text(1:8, par("usr")[3] - 0.25, srt = 45, adj = 1,
     labels = labels, xpd = TRUE)
## @r{Plot x axis label at line 6 (of 7)}
mtext(1, text = "X Axis Label", line = 6)
@end example

@noindent
When plotting the x axis labels, we use @code{srt = 45} for text
rotation angle, @code{adj = 1} to place the right end of text at the
tick marks, and @code{xpd = TRUE} to allow for text outside the plot
region.  You can adjust the value of the @code{0.25} offset as required
to move the axis labels up or down relative to the x axis.  See
@code{?par} for more information.

Also see Figure 1 and associated code in Paul Murrell (2003),
``Integrating grid Graphics Output with Base Graphics Output'',
@emph{R News}, @strong{3/2}, 7--12.

@node Why is read.table() so inefficient?, What is the difference between package and library?, How can I create rotated axis labels?, R Miscellanea
@section Why is read.table() so inefficient?

By default, @code{read.table()} needs to read in everything as character
data, and then try to figure out which variables to convert to numerics
or factors.  For a large data set, this takes condiderable amounts of
time and memory.  Performance can substantially be improved by using the
@code{colClasses} argument to specify the classes to be assumed for the
columns of the table.

@node What is the difference between package and library?, I installed a package but the functions are not there, Why is read.table() so inefficient?, R Miscellanea
@section What is the difference between package and library?

A @dfn{package} is a standardized collection of material extending R,
e.g.@: providing code, data, or documentation.  A @dfn{library} is a
place (directory) where R knows to find packages it can use (i.e., which
were @dfn{installed}).  R is told to use a package (to ``load'' it and
add it to the search path) via calls to the function @code{library}.
I.e., @code{library()} is employed to load a package from libraries
containing packages.

@xref{R Add-On Packages}, for more details.  See also Uwe Ligges (2003),
``R Help Desk: Package Management'', @emph{R News}, @strong{3/3},
37--39.

@node I installed a package but the functions are not there, Why doesn't R think these numbers are equal?, What is the difference between package and library?, R Miscellanea
@section I installed a package but the functions are not there

To actually @emph{use} the package, it needs to be @emph{loaded} using
@code{library()}.

See @ref{R Add-On Packages} and @ref{What is the difference between
package and library?} for more information.

@node Why doesn't R think these numbers are equal?,  , I installed a package but the functions are not there, R Miscellanea
@section Why doesn't R think these numbers are equal?

The only numbers that can be represented exactly in R's numeric type 
are integers and fractions whose denominator is a power of 2.  Other 
numbers have to be rounded to (typically) 53 binary digits accuracy.  As a 
result, two floating point numbers will not reliably be equal unless they 
have been computed by the same algorithm, and not always even then.  For 
example

@example
R> a <- sqrt(2)
R> a * a == 2
[1] FALSE
R> a * a - 2
[1] 4.440892e-16
@end example

The function @code{all.equal()} compares two objects using a numeric
tolerance of @code{.Machine$double.eps ^ 0.5}.  If you want much greater
accuracy than this you will need to consider error propagation
carefully.

@node R Programming, R Bugs, R Miscellanea, Top
@chapter R Programming

@menu
* How should I write summary methods?::  
* How can I debug dynamically loaded code?::  
* How can I inspect R objects when debugging?::  
* How can I change compilation flags?::  
* How can I debug S4 methods?::  
@end menu

@node How should I write summary methods?, How can I debug dynamically loaded code?, R Programming, R Programming
@section How should I write summary methods?

Suppose you want to provide a summary method for class @code{"foo"}.
Then @code{summary.foo()} should not print anything, but return an
object of class @code{"summary.foo"}, @emph{and} you should write a
method @code{print.summary.foo()} which nicely prints the summary
information and invisibly returns its object.  This approach is
preferred over having @code{summary.foo()} print summary information and
return something useful, as sometimes you need to grab something
computed by @code{summary()} inside a function or similar.  In such
cases you don't want anything printed.

@node How can I debug dynamically loaded code?, How can I inspect R objects when debugging?, How should I write summary methods?, R Programming
@section How can I debug dynamically loaded code?

Roughly speaking, you need to start R inside the debugger, load the
code, send an interrupt, and then set the required breakpoints.

@ifclear UseExternalXrefs
See section ``Finding entry points in dynamically loaded code'' in
@cite{Writing R Extensions}.  This manual is included in the R
distribution, @pxref{What documentation exists for R?}.
@end ifclear
@ifset UseExternalXrefs
@xref{Finding entry points, , Finding entry points in dynamically loaded
code, R-exts, Writing R Extensions}.
@end ifset

@node How can I inspect R objects when debugging?, How can I change compilation flags?, How can I debug dynamically loaded code?, R Programming
@section How can I inspect R objects when debugging?

The most convenient way is to call @code{R_PV} from the symbolic
debugger.

@ifclear UseExternalXrefs
See section ``Inspecting R objects when debugging'' in @cite{Writing R
Extensions}.
@end ifclear
@ifset UseExternalXrefs
@xref{Inspecting R objects, , Inspecting R objects when debugging,
R-exts, Writing R Extensions}.
@end ifset

@node How can I change compilation flags?, How can I debug S4 methods?, How can I inspect R objects when debugging?, R Programming
@section How can I change compilation flags?

Suppose you have C code file for dynloading into R, but you want to use
@code{R CMD SHLIB} with compilation flags other than the default ones
(which were determined when R was built).

Starting with R 2.1.0, users can provide personal Makevars configuration
files in @file{$@env{HOME}/.R} to override the default flags.
@ifclear UseExternalXrefs
See section ``Add-on packages'' in @cite{R Installation and
Administration}.
@end ifclear
@ifset UseExternalXrefs
@xref{Add-on packages, , Add-on packages, R-admin,
R Installation and Administration}.
@end ifset

For earlier versions of R, you could change the file
@file{$@env{R_HOME}/etc/Makeconf} to reflect your preferences, or (at
least for systems using @acronym{GNU} Make) override them by the
environment variable @env{MAKEFLAGS}.
@ifclear UseExternalXrefs
See section ``Creating shared objects'' in @cite{Writing R Extensions}.
@end ifclear
@ifset UseExternalXrefs
@xref{Creating shared objects, , Creating shared objects, R-exts,
Writing R Extensions}.
@end ifset

@node How can I debug S4 methods?,  , How can I change compilation flags?, R Programming
@section How can I debug S4 methods?

Use the @code{trace()} function with argument @code{signature=} to add
calls to the browser or any other code to the method that will be
dispatched for the corresponding signature.  See @code{?trace} for
details.

@node R Bugs, Acknowledgments, R Programming, Top
@chapter R Bugs

@menu
* What is a bug?::              
* How to report a bug::         
@end menu

@node What is a bug?, How to report a bug, R Bugs, R Bugs
@section What is a bug?

If R executes an illegal instruction, or dies with an operating system
error message that indicates a problem in the program (as opposed to
something like ``disk full''), then it is certainly a bug.  If you call
@code{.C()}, @code{.Fortran()}, @code{.External()} or @code{.Call()} (or
@code{.Internal()}) yourself (or in a function you wrote), you can
always crash R by using wrong argument types (modes).  This is not a
bug.

Taking forever to complete a command can be a bug, but you must make
certain that it was really R's fault.  Some commands simply take a long
time.  If the input was such that you @emph{know} it should have been
processed quickly, report a bug.  If you don't know whether the command
should take a long time, find out by looking in the manual or by asking
for assistance.

If a command you are familiar with causes an R error message in a case
where its usual definition ought to be reasonable, it is probably a bug.
If a command does the wrong thing, that is a bug.  But be sure you know
for certain what it ought to have done.  If you aren't familiar with the
command, or don't know for certain how the command is supposed to work,
then it might actually be working right.  Rather than jumping to
conclusions, show the problem to someone who knows for certain.

Finally, a command's intended definition may not be best for statistical
analysis.  This is a very important sort of problem, but it is also a
matter of judgment.  Also, it is easy to come to such a conclusion out
of ignorance of some of the existing features.  It is probably best not
to complain about such a problem until you have checked the
documentation in the usual ways, feel confident that you understand it,
and know for certain that what you want is not available.  If you are
not sure what the command is supposed to do after a careful reading of
the manual this indicates a bug in the manual.  The manual's job is to
make everything clear.  It is just as important to report documentation
bugs as program bugs.  However, we know that the introductory
documentation is seriously inadequate, so you don't need to report this.

If the online argument list of a function disagrees with the manual, one
of them must be wrong, so report the bug.

@node How to report a bug,  , What is a bug?, R Bugs
@section How to report a bug

When you decide that there is a bug, it is important to report it and to
report it in a way which is useful.  What is most useful is an exact
description of what commands you type, starting with the shell command
to run R, until the problem happens.  Always include the version of R,
machine, and operating system that you are using; type @kbd{version} in
R to print this.

The most important principle in reporting a bug is to report
@emph{facts}, not hypotheses or categorizations.  It is always easier to
report the facts, but people seem to prefer to strain to posit
explanations and report them instead.  If the explanations are based on
guesses about how R is implemented, they will be useless; others will
have to try to figure out what the facts must have been to lead to such
speculations.  Sometimes this is impossible.  But in any case, it is
unnecessary work for the ones trying to fix the problem.

For example, suppose that on a data set which you know to be quite large
the command

@example
R> data.frame(x, y, z, monday, tuesday)
@end example

@noindent
never returns.  Do not report that @code{data.frame()} fails for large
data sets.  Perhaps it fails when a variable name is a day of the week.
If this is so then when others got your report they would try out the
@code{data.frame()} command on a large data set, probably with no day of
the week variable name, and not see any problem.  There is no way in the
world that others could guess that they should try a day of the week
variable name.

Or perhaps the command fails because the last command you used was a
method for @code{"["()} that had a bug causing R's internal data
structures to be corrupted and making the @code{data.frame()} command
fail from then on.  This is why others need to know what other commands
you have typed (or read from your startup file).

It is very useful to try and find simple examples that produce
apparently the same bug, and somewhat useful to find simple examples
that might be expected to produce the bug but actually do not.  If you
want to debug the problem and find exactly what caused it, that is
wonderful.  You should still report the facts as well as any
explanations or solutions.  Please include an example that reproduces
the problem, preferably the simplest one you have found.

Invoking R with the @option{--vanilla} option may help in isolating a
bug.  This ensures that the site profile and saved data files are not
read.

Before you actually submit a bug report, you should check whether the
bug has already been reported and/or fixed.  First, try the ``Search
Existing Reports'' facility in the Bug Tracking page at
@url{http://bugs.R-project.org/}.  Second, consult
@url{https://svn.R-project.org/R/trunk/NEWS}, which records changes that
will appear in the @emph{next} release of R, including some bug fixes
that do not appear in Bug Tracking.  (Windows users should additionally
consult @url{https://svn.R-project.org/R/trunk/src/gnuwin32/CHANGES}.)
Third, if possible try the current r-patched or r-devel version of R.
If a bug has already been reported or fixed, please do not submit
further bug reports on it.

On Unix systems a bug report can be generated using the function
@code{bug.report()}.  This automatically includes the version
information and sends the bug to the correct address.  Alternatively the
bug report can be emailed to @email{R-bugs@@R-project.org} or submitted
to the Web page at @url{http://bugs.R-project.org/}.

Bug reports on contributed packages should be sent first to the package
maintainer, and only submitted to the R-bugs repository by package
maintainers, mentioning the package in the subject line.

There is a section of the bug repository for suggestions for
enhancements for R labelled @samp{wishlist}.  Suggestions can be
submitted in the same ways as bugs, but please ensure that the subject
line makes clear that this is for the wishlist and not a bug report, for
example by starting with @samp{Wishlist:}.

Comments on and suggestions for the Windows port of R should be sent to
@email{R-windows@@R-project.org}.

Corrections to and comments on message translation should be sent to the
last translator (listed at the top of the appropriate @samp{.po} file)
or to the translation team as listed at
@url{http://developer.r-project.org/TranslationTeams.html}.

@node Acknowledgments,  , R Bugs, Top
@chapter Acknowledgments

Of course, many many thanks to Robert and Ross for the R system, and to
the package writers and porters for adding to it.

Special thanks go to Doug Bates, Peter Dalgaard, Paul Gilbert, Stefano
Iacus, Fritz Leisch, Jim Lindsey, Thomas Lumley, Martin Maechler, Brian
D. Ripley, Anthony Rossini, and Andreas Weingessel for their comments
which helped me improve this @acronym{FAQ}.

More to some soon @dots{}

@bye

@c Local Variables: ***
@c mode: TeXinfo ***
@c End: ***
