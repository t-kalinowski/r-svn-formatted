\documentclass[a4paper]{article}

\usepackage{Rd, parskip, amsmath, enumerate}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{color}
\definecolor{Blue}{rgb}{0,0,0.8}
\hypersetup{%
colorlinks,%
plainpages=true,%
linkcolor=black,%
citecolor=black,%
urlcolor=Blue,%
%pdfstartview=FitH,% or Fit
pdfstartview={XYZ null null 1},%
pdfview={XYZ null null null},%
pdfpagemode=UseNone,% for no outline
pdfauthor={R-core}%
}

%\VignetteIndexEntry{Package 'parallel'}
%\VignettePackage{parallel}


\title{Package `parallel'}
\author{R-core}


\begin{document}

\maketitle

\section{Introduction}

A package \pkg{parallel} is being developed for release in \R{} 2.14.0.
It will build on the work done for CRAN packages \pkg{multicore}
\citep{multicore} and \pkg{snow} \citep{snow}.

Until October 2011 anything in this package is strictly experimental
and may be changed or removed without notice.

Parallelism can be done in computation at many different levels: this
package is principally concerned with `coarse-grained
parallelization'.  At the lowest level, modern CPUs can do several
basic operations in parallel (e.g.{} integer and floating-point
arithmetic), and several implementations of external BLAS libraries
use multiple threads to do parts of basic vector/matrix operations in
parallel.

This package is about running much larger chunks of computations in
parallel.  A typical example is to evaluate the same \R{} function on
many different sets of data: often simulated data as in bootstrap
computations (or with `data' being the random-number stream).  The
crucial point is that these chunks of computation are unrelated and do
not need to communicate in any way.  It is often important that the
chunks take approximately the same length of time.  The basic
computational model is
\begin{enumerate}[(a)]
\item Start up $M$ `worker' processes, and do any initialization
  needed on the workers.
\item Send any data required to the workers.
\item Split the task into $M$ roughly equally-sized chunks, and send
  the chunks (including the \R{} code needed) to the workers.
\item Wait for all the workers to complete their tasks, and ask them
  for their results.
\item Repeat steps (b--d) for any further tasks.
\item Shut down the worker processes.
\end{enumerate}
Amongst the initializations which may be needed are to load packages
and initialize the random-number stream.

A slightly different model is to split the task into $M_1 > M$ chunks,
send the first $M$ chunks to the workers, then repeatedly wait for any
worker to complete and send it the next remaining task: this is done
in \code{mclapply(mc.preschedule = FALSE)} and \code{clusterApplyLB}.

In principle the workers could be implemented by threads or
lightweight processes, but in the current implementation they are full
processes.  They can be created in one of three ways:
\begin{enumerate}
\item \emph{Via} forking.  \emph{Fork} is a
  concept\footnote{\url{http://en.wikipedia.org/wiki/Fork_(operating_system)}}
  from POSIX operating systems, and should be available on all \R{}
  platforms except Windows.  This creates a new \R{} process by taking
  a complete copy of the master process, including the workspace and
  state of the random-number stream.  However, the copy will usually
  share pages with the master until modified so forking is very fast.

  The use of forking was pioneered by package \pkg{multicore}.

  Note that as it does share the complete process, it also shares any
  GUI elements, for example an \R{} console and on-screen devices.
  This can cause havoc.

  There needs to be a way to communicate between master and worker.
  Once again there are several possibilities since master and workers
  share memory. In \pkg{multicore} the initial fork sends an \R{}
  expression to be evaluated to the worker, and the master process
  opens a pipe for reading that is used by the worker to return the
  results.

\item \emph{Via} \code{system("Rscript")} or similar to launch a new
  process on the current machine or a similar machine with an identical
  \R installation.  This then needs a way to communicate between
  master and worker processes, which is usually done \emph{via}
  sockets.

  This should be available on all \R{} platforms, although it is
  conceivable that zealous security measures could block the
  inter-process communication \emph{via} sockets.

\item Using OS-level facilities to set up a means to send tasks to
  other members of a group of machines.  There are several ways to do
  that, and for example package \pkg{snow} can make use of PVM
  (`parallel virtual machine') and MPI (`message passing interface')
  using \R{} packages \pkg{rpvm} and \pkg{Rmpi} respectively.
  Communication overheads can dominate computation times in this
  approach, so it is most often used on tightly-coupled clusters of
  computers with high-speed interconnects.  It will not be considered
  further in this package.
\end{enumerate}

The landscape of parallel computing has changed with the advent of
shared-memory computers with multiple (and often many) CPU cores.
Until the late 2000's parallel computing was mainly done on clusters
of large numbers of single- or dual-CPU computers: nowadays even
laptops have two or four cores, and servers with 8, 12 or more are
commonplace.  It is such hardware that package \pkg{parallel} is
designed to exploit.

%<<>>=
%library(parallel)
%@

\section{Numbers of CPUs}

In setting up parallel computations it can be helpful to have some
idea of the number of CPUs available, but this is a rather slippery
concept.

Note that all a program can possibly determine is the total number of
CPUs available.  This is not the same as the number of CPUs available
\emph{to the current user} which may well be restricted by system
policies on multi-user systems.  Nor does it give much idea of a
reasonable number of CPUs to use for the current task: the user may be
running many \R{} processes simultaneously, and those processes may
themselves by using multiple threads through multi-threaded BLAS,
compiled code using OpenMP or other low-level forms of parallelism.
We have even seen instances of \pkg{multicore}'s \code{mclapply} being
called recursively, generating $2n + n^2$ processes on a machine
estimated to have $n = 16$ CPUs.

But in so far as it is a helpful guideline, function
\code{detectCores()} try to determine the number of CPU cores in the
machine on which \R{} is running: it has ways to do so on all known
current \R{} platforms.  What exactly it measures is OS-specific: we
try where possible to report the number of physical cores available.
One OS where this is not the case is Windows, where only the number
of logical CPUs is readily available.  On modern hardware (e.g.{}
Intel \emph{Core i7}) the latter may not be unreasonable as
hyper-threading does give a significant extra throughput: on what are
now rather old Pentium 4's it will simply over-estimate by a factor of
two.  Note that we said `available': in most cases when virtual
machines are in use what is reported is the maximum number of virtual
CPUs which can be used simultaneously by that VM.

\section{Socket Clusters}

Currently the package contains a copy of much of \pkg{snow}, and the
functions it contains can also be used with SNOW clusters.

The implementation is limited to socket clusters created by function
\code{makePSOCKcluster}, which in this package is identical to
\code{makeCluster}.  The only current differences are in the fine
details in how new \R{} processes are created.

\section{Forking}

Except on Windows, the package currently contains a copy of the low-
and intermediate-level interfaces from \pkg{multicore}: there a few
names with the added prefix \code{mc}, e.g.{} \code{mccollect},
\code{mcexit}, \code{mcfork}, \code{mckill} and \code{mcparallel}.

Most of the functions provided are not intended to be used by end
users, and they may not in future be exported from the namespace.

There are also copies of the higher-level functions \code{mclapply}
and \code{pvec}: unlike the versions in \pkg{multicore} these default
to 2 cores, but this can be controlled by setting
\code{options("mc.cores")}, and that takes its default from
environment variable \code{MC\_CORES} when the package is loaded.

\section{Random-number generation}

Some care is needed with parallel computation using (pseudo-)random
numbers: the processes/threads which run separate parts of the
computation need to run independent (and preferably reproducible)
random-number streams.  One way to avoid any difficulties is (where
possible) to do all the randomization in the master process: this is
done in package \pkg{boot} (version 1.3-1 and later).

When an \R{} process is started up it takes the random-number seed
from the object \code{.Random.seed} in a saved workspace or constructs
one from the clock time (see the help on \code{RNG}: as from \R{}
2.14.0 it also uses the process ID).  Thus worker processes might get
the same seed, either because a workspace was restored or (in \R{} <
2.14.0) because the workers were started at the same time: otherwise
these get a non-reproducible seed.

The alternative is to set separate seeds for each worker process in
some reproducible way from the seed in the master process.  This is
generally perfectly safe, but there have been worries that the
random-number streams in the workers might somehow get into step.  One
approach is to take the seeds a long way apart in the random-number
stream: note that random numbers taken a long (fixed) distance apart
in a single stream are not necessarily (and often are not) as
independent as those a short distance apart.  Yet another idea (as
used by e.g.{} \pkg{JAGS}) is to use different random-number
generators for each separate run/process.

Package \pkg{parallel} contains an implementation of the ideas of
\citet{lecuyer.2002}: this uses a single RNG and takes seeds $2^{127}$
apart in the random number stream (which has period approximately
$2^{191}$).  This is based on the generator of \citet{lecuyer.1999};
the reason for choosing that generator\footnote{apart from the
  commonality of authors!} is that it has a fairly long period with a
small seed (6 integers), and unlike \R{}'s default
\code{"Mersenne-Twister"} RNG, it is simple to advance the seed by a
fixed number of steps.  The generator is the combination of two:
\begin{eqnarray*}
  x_n &=& 1403580 \times x_{n-1} - 810728 \times x_{n-3} \mod{(2^{32} - 209)}\\
  y_n &=& 527612 \times y_{n-1} - 1370589 \times y_{n-3} \mod{(2^{32} - 22853)}\\
  z_n &=& (x_n - y_n) \mod{4294967087}\\
  u_n &=& z_n/4294967088\ \mbox{unless $z_n = 0$}
\end{eqnarray*}
The `seed' then consists of $(x_n, x_{n-1}, x_{n-2}, y_n, y_{n-1},
y_{n-2})$, and the recursion for each of $x_n$ and $y_n$ can have
pre-computed coefficients for $k$ steps ahead.  For $k = 2^{127}$, the
seed is advanced by $k$ steps by \R{} function
\code{nextRNGStream(.Random.seed)}.

The \citet{lecuyer.1999} generator is available in \R{} as from
version 2.14.0 \emph{via} \code{RNGkind("L'Ecuyer-CMRG")}.  Thus using
the ideas of \citet{lecuyer.2002} is as simple as
\begin{verbatim}
RNGkind("L'Ecuyer-CMRG")
set.seed(<something>)
## start M workers
s <- .Random.seed
for (i in 1:M) {
    s <- nextRNGStream(s)
    # send s to worker i as .Random.seed
}
\end{verbatim}
and this is is implemented for SNOW clusters in function
\code{clusterSetRNGStream}, and as part of \code{mcparallel} and
\code{mclapply}.

Apart from \emph{streams} ($2^{127}$ apart), there is the concept of
\emph{sub-streams} starting from seeds $2^{76}$ steps apart.

A direct \R{} interface to the (clunkier) original C implementation is
available in CRAN package \pkg{rlecuyer} \citep{rlecuyer}.  That works
with named streams, each of which have three 6-elements seeds
associated with them, the original seed set for the package, the
initial seed set for the stream and the current seed for the stream.
This can easily be emulated in \R{} by storing \code{.Random.seed} at
suitable times.  There is another interface using S4 classes in
package \pkg{rstream} \citep{rstream}.

\bibliographystyle{jss}
\bibliography{parallel}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
